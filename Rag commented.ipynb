{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import ir_datasets\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "api_key = \"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\"\n",
    "login(token=api_key)\n",
    "\n",
    "# Check GPU availability\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        gpu_properties = torch.cuda.get_device_properties(torch.cuda.current_device())\n",
    "        print(f\"Using GPU: {gpu_properties.name}\")\n",
    "        print(f\"CUDA Cores: {gpu_properties.multi_processor_count}\")\n",
    "        print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading the TREC-COVID dataset.\")\n",
    "dataset = ir_datasets.load(\"cord19/trec-covid\")\n",
    "\n",
    "# Prepare documents and queries\n",
    "print(\"Preparing documents and queries.\")\n",
    "all_docs = [{\"doc_id\": doc.doc_id, \"abstract\": doc.abstract} for doc in dataset.docs_iter()]\n",
    "all_queries = [{\"query_id\": query.query_id, \"title\": query.title} for query in dataset.queries_iter()]\n",
    "\n",
    "# Print dataset size information\n",
    "print(f\"Summary: {len(all_docs)} documents and {len(all_queries)} queries are available in the dataset.\")\n",
    "\n",
    "# Tokenize the abstracts of all documents for further processing\n",
    "tokenized_docs = [doc['abstract'].split() for doc in all_docs]\n",
    "qrels = dataset.qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load or generate embeddings for documents and queries\n",
    "def generate_embeddings():\n",
    "    # Check if precomputed embeddings exist as CSV files, and if so, load them\n",
    "    if os.path.exists(\"trec_covid_doc_embeddings.csv\") and os.path.exists(\"trec_covid_query_embeddings.csv\"):\n",
    "        print(\"Loading precomputed embeddings.\")\n",
    "        doc_embeddings = pd.read_csv(\"trec_covid_doc_embeddings.csv\").values\n",
    "        query_embeddings = pd.read_csv(\"trec_covid_query_embeddings.csv\").values\n",
    "    else:\n",
    "        print(\"No precomputed embeddings found.\")\n",
    "        print(\"Generating new embeddings using SentenceTransformer model 'all-MiniLM-L6-v2'.\")\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "        # Generate dense embeddings for all documents and queries\n",
    "        doc_embeddings = model.encode(all_docs, batch_size=32, show_progress_bar=True)\n",
    "        query_embeddings = model.encode(all_queries, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "        # Save the generated embeddings as CSV files for future use\n",
    "        pd.DataFrame(doc_embeddings).to_csv(\"trec_covid_doc_embeddings.csv\", index=False)\n",
    "        pd.DataFrame(query_embeddings).to_csv(\"trec_covid_query_embeddings.csv\", index=False)\n",
    "\n",
    "    return doc_embeddings, query_embeddings\n",
    "\n",
    "\n",
    "doc_embeddings, query_embeddings = generate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load or generate embeddings for documents and queries\n",
    "def generate_embeddings():\n",
    "    # Check if precomputed embeddings exist as CSV files, and if so, load them\n",
    "    if os.path.exists(\"trec_covid_doc_embeddings.csv\") and os.path.exists(\"trec_covid_query_embeddings.csv\"):\n",
    "        print(\"Loading precomputed embeddings.\")\n",
    "        doc_embeddings = pd.read_csv(\"trec_covid_doc_embeddings.csv\").values\n",
    "        query_embeddings = pd.read_csv(\"trec_covid_query_embeddings.csv\").values\n",
    "    else:\n",
    "        print(\"No precomputed embeddings found.\")\n",
    "        print(\"Generating new embeddings using SentenceTransformer model 'all-MiniLM-L6-v2'.\")\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "        # Generate dense embeddings for all documents and queries\n",
    "        doc_embeddings = model.encode(all_docs, batch_size=32, show_progress_bar=True)\n",
    "        query_embeddings = model.encode(all_queries, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "        # Save the generated embeddings as CSV files for future use\n",
    "        pd.DataFrame(doc_embeddings).to_csv(\"trec_covid_doc_embeddings.csv\", index=False)\n",
    "        pd.DataFrame(query_embeddings).to_csv(\"trec_covid_query_embeddings.csv\", index=False)\n",
    "\n",
    "    return doc_embeddings, query_embeddings\n",
    "\n",
    "\n",
    "doc_embeddings, query_embeddings = generate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def dcg_at_k(scores, k):\n",
    "    \"\"\"Calculates the Discounted Cumulative Gain (DCG) at rank position k.\"\"\"\n",
    "    if not scores or k <= 0:  # Handle empty input or invalid k\n",
    "        return 0.0\n",
    "    # Select top-k scores for evaluation\n",
    "    scores = np.array(scores)[:k]\n",
    "    # Calculate the discounts for each position in the ranking (logarithmic scaling)\n",
    "    discounts = np.log2(np.arange(2, len(scores) + 2))\n",
    "    # Return the sum of the discounted cumulative gains\n",
    "    return np.sum(scores / discounts)\n",
    "\n",
    "\n",
    "def ndcg_at_k(retrieved_scores, ideal_scores, k):\n",
    "    \"\"\"Computes the Normalized Discounted Cumulative Gain (nDCG) at rank k.\"\"\"\n",
    "    if not retrieved_scores or not ideal_scores or k <= 0:\n",
    "        return 0.0\n",
    "    # Calculate the DCG of the retrieved documents\n",
    "    dcg = dcg_at_k(retrieved_scores, k)\n",
    "    # Calculate the ideal DCG by sorting the ideal scores in descending order\n",
    "    idcg = dcg_at_k(sorted(ideal_scores, reverse=True), k)\n",
    "    # Return the normalized DCG value\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def recall_at_k(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"Calculates Recall at rank k.\"\"\"\n",
    "    if not relevant_docs or k <= 0:  # Handle empty relevant_docs or invalid k\n",
    "        return 0.0\n",
    "    # Select the top-k retrieved document IDs\n",
    "    retrieved_at_k = set(retrieved_docs[:k])\n",
    "    # Calculate the recall as the ratio of relevant documents retrieved in the top-k\n",
    "    return len(retrieved_at_k & set(relevant_docs)) / len(relevant_docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# BM25 Sparse Retrieval\n",
    "def bm25_retrieve(query, bm25, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform sparse retrieval using BM25 on the tokenized documents.\n",
    "    Returns the indices and scores of the top-k documents.\n",
    "    \"\"\"\n",
    "    tokenized_query = query.split()                                             # Tokenize the query into words\n",
    "    scores = bm25.get_scores(tokenized_query)                                   # Get BM25 scores for all documents\n",
    "    top_k_indices = np.argsort(scores)[-top_k:][::-1]                           # Get indices of top-k documents based on BM25 score\n",
    "    return top_k_indices, scores[top_k_indices]\n",
    "\n",
    "# Dense Retrieval\n",
    "def dense_retrieve(query_embedding, doc_embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform dense retrieval using cosine similarity between query and document embeddings.\n",
    "    Returns the indices and similarities of the top-k documents.\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]      # Compute cosine similarity\n",
    "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]                     # Get top-k indices based on similarity\n",
    "    return top_k_indices, similarities[top_k_indices]\n",
    "\n",
    "# Rank Fusion Retrieval\n",
    "def fusion_retrieve(dense_query_embedding, doc_embeddings, query, top_k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Implementa il rank fusion riutilizzando le funzioni esistenti di retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform BM25 retrieval and dense retrieval\n",
    "    sparse_indices, sparse_scores = bm25_retrieve(query, bm25, top_k=len(doc_embeddings))\n",
    "    dense_indices, dense_scores = dense_retrieve(dense_query_embedding, doc_embeddings, top_k=len(doc_embeddings))\n",
    "\n",
    "    # Initialize score arrays\n",
    "    all_sparse_scores = np.zeros(len(doc_embeddings))\n",
    "    all_dense_scores = np.zeros(len(doc_embeddings))\n",
    "    \n",
    "    # Fill score arrays with BM25 and dense scores\n",
    "    all_sparse_scores[sparse_indices] = sparse_scores\n",
    "    all_dense_scores[dense_indices] = dense_scores\n",
    "    \n",
    "    # Normalize scores    \n",
    "    all_sparse_scores = (all_sparse_scores - all_sparse_scores.min()) / (all_sparse_scores.max() - all_sparse_scores.min())\n",
    "    all_dense_scores = (all_dense_scores - all_dense_scores.min()) / (all_dense_scores.max() - all_dense_scores.min())\n",
    "    \n",
    "    # Combine scores using the alpha parameter\n",
    "    combined_scores = alpha * all_dense_scores + (1 - alpha) * all_sparse_scores\n",
    "    \n",
    "    # Retrieve the top-k results based on combined scores\n",
    "    top_k_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "    return top_k_indices, combined_scores[top_k_indices]\n",
    "\n",
    "# Cascading Retrieval\n",
    "def cascade_retrieve(dense_query_embedding, doc_embeddings, query, initial_k=100, final_k=5, dense_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Perform cascading retrieval: sparse retrieval followed by dense re-ranking.\n",
    "    Filters documents based on a similarity threshold and returns the top-k results.\n",
    "    \"\"\"\n",
    "    # Stage 1: BM25 to get initial candidates\n",
    "    initial_indices, _ = bm25_retrieve(query, bm25, top_k=initial_k)\n",
    "    \n",
    "    # Stage 2: Dense re-ranking of candidate documents\n",
    "    candidate_embeddings = doc_embeddings[initial_indices]\n",
    "    _, dense_scores = dense_retrieve(dense_query_embedding, candidate_embeddings, top_k=len(initial_indices))\n",
    "    \n",
    "    # Filter candidates by similarity threshold\n",
    "    qualified_mask = dense_scores >= dense_threshold\n",
    "    if np.sum(qualified_mask) >= final_k:\n",
    "        # Select top-k qualified candidates\n",
    "        qualified_indices = np.where(qualified_mask)[0]\n",
    "        top_indices = qualified_indices[np.argsort(dense_scores[qualified_indices])[-final_k:][::-1]]\n",
    "    else:\n",
    "        # If there are not enough qualified candidates, select top-k by overall scores\n",
    "        top_indices = np.argsort(dense_scores)[-final_k:][::-1]\n",
    "    \n",
    "    # Map filtered indices to original document IDs\n",
    "    final_indices = initial_indices[top_indices]\n",
    "    final_scores = dense_scores[top_indices]\n",
    "    \n",
    "    return final_indices, final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize BM25 model\n",
    "print(\"Initializing BM25 model.\")\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Run retrieval experiments\n",
    "def run_retrieval_experiments():\n",
    "    \"\"\"\n",
    "    Execute sparse, dense, rank fusion, and cascading retrieval for all queries.\n",
    "    Save the results to a JSON file for further analysis.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"sparse\": [],\n",
    "        \"dense\": [],\n",
    "        \"rank_fusion\": [],\n",
    "        \"cascade\": []\n",
    "    }\n",
    "    metrics = {\n",
    "        \"sparse\": [],\n",
    "        \"dense\": [],\n",
    "        \"rank_fusion\": [],\n",
    "        \"cascade\": []\n",
    "    }\n",
    "\n",
    "    print(\"Running retrieval experiments on all queries.\")\n",
    "\n",
    "    # Iterate over each query and its embedding\n",
    "    for query, query_embedding in tqdm(zip(all_queries, query_embeddings), total=len(all_queries)):\n",
    "        # Extract the query ID and text for the current query\n",
    "        query_id = query['query_id']\n",
    "        query_text = query['title'] if isinstance(query, dict) else query\n",
    "\n",
    "        # Get the set of relevant document IDs for the current query based on the relevance annotations in 'qrels'\n",
    "        relevant_docs = {qrel.doc_id for qrel in qrels if qrel.query_id == query_id and qrel.relevance > 0}\n",
    "        ideal_scores = [qrel.relevance for qrel in qrels if qrel.query_id == query_id and qrel.relevance > 0]\n",
    "\n",
    "        \n",
    "        # Sparse Retrieval using BM25\n",
    "        sparse_indices, sparse_scores = bm25_retrieve(query_text, bm25)                 # Retrieve the top-k BM25 documents and their scores\n",
    "        sparse_docs = [all_docs[idx]['doc_id'] for idx in sparse_indices]               # Get document IDs from the indices\n",
    "        results[\"sparse\"].append({\"query\": query, \"results\": sparse_docs})              # Store the BM25 results for the current query\n",
    "        \n",
    "        sparse_dcg = dcg_at_k(sparse_scores, k=5)\n",
    "        sparse_ndcg = ndcg_at_k(sparse_scores, ideal_scores, k=5)\n",
    "        sparse_recall = recall_at_k(sparse_docs, relevant_docs, k=5)\n",
    "        metrics[\"sparse\"].append({\"dcg@5\": sparse_dcg, \"ndcg@5\": sparse_ndcg, \"recall@5\": sparse_recall})\n",
    "\n",
    "        # Dense Retrieval using cosine similarity\n",
    "        dense_indices, dense_scores = dense_retrieve(query_embedding, doc_embeddings)   # Retrieve the top-k documents based on cosine similarity of embeddings\n",
    "        dense_docs = [all_docs[idx]['doc_id'] for idx in dense_indices]\n",
    "        results[\"dense\"].append({\"query\": query, \"results\": dense_docs})\n",
    "        \n",
    "        dense_dcg = dcg_at_k(dense_scores, k=5)\n",
    "        dense_ndcg = ndcg_at_k(dense_scores, ideal_scores, k=5)\n",
    "        dense_recall = recall_at_k(dense_docs, relevant_docs, k=5)\n",
    "        metrics[\"dense\"].append({\"dcg@5\": dense_dcg, \"ndcg@5\": dense_ndcg, \"recall@5\": dense_recall})\n",
    "\n",
    "        # Rank Fusion Retrieval by combining sparse (BM25) and dense result\n",
    "        fusion_indices, fusion_scores = fusion_retrieve(                                # Combine BM25 and cosine similarity results\n",
    "            query_embedding, doc_embeddings, query_text\n",
    "        )\n",
    "        fusion_docs = [all_docs[idx]['doc_id'] for idx in fusion_indices]\n",
    "        results[\"rank_fusion\"].append({\"query\": query, \"results\": fusion_docs})\n",
    "        \n",
    "        fusion_dcg = dcg_at_k(fusion_scores, k=5)\n",
    "        fusion_ndcg = ndcg_at_k(fusion_scores, ideal_scores, k=5)\n",
    "        fusion_recall = recall_at_k(fusion_docs, relevant_docs, k=5)\n",
    "        metrics[\"rank_fusion\"].append({\"dcg@5\": fusion_dcg, \"ndcg@5\": fusion_ndcg, \"recall@5\": fusion_recall})\n",
    "\n",
    "        # Cascade Retrieval: First use BM25, then re-rank using dense retrieval\n",
    "        cascade_indices, cascade_scores = cascade_retrieve(                             # Perform cascading retrieval\n",
    "            query_embedding, doc_embeddings, query_text\n",
    "        )\n",
    "        cascade_docs = [all_docs[idx]['doc_id'] for idx in cascade_indices]\n",
    "        results[\"cascade\"].append({\"query\": query, \"results\": cascade_docs})\n",
    "        \n",
    "        cascade_dcg = dcg_at_k(cascade_scores, k=5)\n",
    "        cascade_ndcg = ndcg_at_k(cascade_scores, ideal_scores, k=5)\n",
    "        cascade_recall = recall_at_k(cascade_docs, relevant_docs, k=5)\n",
    "        metrics[\"cascade\"].append({\"dcg@5\": cascade_dcg, \"ndcg@5\": cascade_ndcg, \"recall@5\": cascade_recall})\n",
    "\n",
    "    # Save results and metrics to JSON files\n",
    "    print(\"Saving results and metrics.\")\n",
    "    with open(\"retrieval_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    with open(\"retrieval_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(\"Retrieval results and metrics saved to files.\")\n",
    "    \n",
    "    return results, metrics\n",
    "\n",
    "\n",
    "run_retrieval_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# QA for the first query\n",
    "QUERY_INDEX = 3                                                     # Index of the query to be used for retrieval\n",
    "query = all_queries[QUERY_INDEX - 1]                                # Select the query from the list based on the index\n",
    "query_text = query['title'] if isinstance(query, dict) else query   # Get the query text\n",
    "\n",
    "# Retrieval calls:\n",
    "\n",
    "# Perform dense retrieval using query embedding and document embeddings\n",
    "dense_top_k_indices, dense_top_k_scores = dense_retrieve(query_embeddings[QUERY_INDEX], doc_embeddings)\n",
    "# Perform sparse retrieval using BM25 on the query text\n",
    "sparse_top_k_indices, sparse_top_k_scores = bm25_retrieve(query_text, bm25)\n",
    "# Perform rank fusion retrieval by combining BM25 and dense retrieval results\n",
    "rank_top_k_indices, rank_top_k_scores = fusion_retrieve(\n",
    "    query_embeddings[QUERY_INDEX], \n",
    "    doc_embeddings, \n",
    "    query_text\n",
    ")\n",
    "# Perform cascading retrieval: first BM25, then re-rank with dense retrieval\n",
    "cascading_top_k_indices, cascading_top_k_scores = cascade_retrieve(\n",
    "    query_embeddings[QUERY_INDEX], \n",
    "    doc_embeddings, \n",
    "    query_text\n",
    ")\n",
    "\n",
    "# Get retrieved documents for each method\n",
    "dense_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(dense_top_k_indices)]\n",
    "sparse_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(sparse_top_k_indices)]\n",
    "rank_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(rank_top_k_indices)]\n",
    "cascading_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(cascading_top_k_indices)]\n",
    "\n",
    "# Definition of the model that will be used to generate the various responses.\n",
    "lm_pipeline = pipeline(\"text-generation\", \n",
    "                      model=\"meta-llama/Llama-3.2-1B\",\n",
    "                      device=0 if device == \"cuda\" else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"------------------ DENSE RETRIEVAL ----------------------\\n\")\n",
    "context = \"\\n\".join(dense_retrieved_docs)\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer:\"\n",
    "\n",
    "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
    "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
    "\n",
    "# Generate response\n",
    "response = lm_pipeline(prompt, \n",
    "                      max_new_tokens=150, \n",
    "                      temperature=0.1, \n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "response = response.split(\"Answer:\")[1].strip()\n",
    "\n",
    "print(f\"------------------ Response ------------------\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"------------------ SPARSE RETRIEVAL ----------------------\\n\")\n",
    "context = \"\\n\".join(sparse_retrieved_docs)\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer:\"\n",
    "\n",
    "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
    "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
    "\n",
    "# Generate response\n",
    "response = lm_pipeline(prompt, \n",
    "                      max_new_tokens=150, \n",
    "                      temperature=0.1, \n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "\n",
    "response = response.split(\"Answer:\")[1].strip()\n",
    "print(f\"------------------ Response ------------------\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"------------------ RANK FUSION ----------------------\\n\")\n",
    "context = \"\\n\".join(rank_retrieved_docs)\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\"\n",
    "\n",
    "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
    "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
    "\n",
    "# Generate response\n",
    "response = lm_pipeline(prompt, \n",
    "                      max_new_tokens=150, \n",
    "                      temperature=0.1, \n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "\n",
    "response = response.split(\"Answer:\")[1].strip()\n",
    "print(f\"------------------ Response ------------------\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"------------------ CASCADING RETRIEVAL ----------------------\\n\")\n",
    "context = \"\\n\".join(cascading_retrieved_docs)\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer:\"\n",
    "\n",
    "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
    "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
    "\n",
    "# Generate response\n",
    "response = lm_pipeline(prompt, \n",
    "                      max_new_tokens=150, \n",
    "                      temperature=0.7, \n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "\n",
    "response = response.split(\"Answer:\")[1].strip()\n",
    "print(f\"------------------ Response ------------------\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"------------------ RESPONSE WITHOUT RAG ----------------------\\n\")\n",
    "prompt = f\"\"\"Question:\\n{query_text}\\n\\nAnswer:\"\"\"\n",
    "\n",
    "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
    "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
    "\n",
    "response = lm_pipeline(prompt,\n",
    "                      max_new_tokens=150,\n",
    "                      temperature=0.1,\n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "\n",
    "response = response.split(\"Answer:\")[1].strip()\n",
    "print(f\"------------------ Response ------------------\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
