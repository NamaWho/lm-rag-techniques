{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import ir_datasets\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "api_key = \"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\"\n",
    "login(token=api_key)\n",
    "\n",
    "# Check GPU availability\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SECTION 1: DATASET LOADING AND PREPARATION\n",
    "# Load dataset\n",
    "print(\"Loading the Vaswani dataset...\")\n",
    "dataset = ir_datasets.load(\"cord19/trec-covid\")\n",
    "\n",
    "# Prepare documents and queries\n",
    "print(\"Preparing documents and queries...\")\n",
    "all_docs = [{\"doc_id\": doc.doc_id, \"abstract\": doc.abstract} for doc in dataset.docs_iter()]\n",
    "all_queries = [{\"query_id\": query.query_id, \"title\": query.title} for query in dataset.queries_iter()]\n",
    "\n",
    "tokenized_docs = [doc['abstract'].split() for doc in all_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SECTION 2: EMBEDDINGS GENERATION\n",
    "# Load or generate embeddings\n",
    "def generate_embeddings():\n",
    "    if os.path.exists(\"trec_covid_doc_embeddings.csv\") and os.path.exists(\"trec_covid_query_embeddings.csv\"):\n",
    "        print(\"Loading precomputed embeddings...\")\n",
    "        doc_embeddings = pd.read_csv(\"trec_covid_doc_embeddings.csv\").values\n",
    "        query_embeddings = pd.read_csv(\"trec_covid_query_embeddings.csv\").values\n",
    "    else:\n",
    "        print(\"Generating embeddings with SentenceTransformer...\")\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "        doc_embeddings = model.encode(all_docs, batch_size=32, show_progress_bar=True)\n",
    "        query_embeddings = model.encode(all_queries, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "        # Save embeddings for future use\n",
    "        pd.DataFrame(doc_embeddings).to_csv(\"trec_covid_doc_embeddings.csv\", index=False)\n",
    "        pd.DataFrame(query_embeddings).to_csv(\"trec_covid_query_embeddings.csv\", index=False)\n",
    "\n",
    "    return doc_embeddings, query_embeddings\n",
    "\n",
    "doc_embeddings, query_embeddings = generate_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SECTION 3: RETRIEVAL IMPLEMENTATION\n",
    "# BM25 Sparse Retrieval\n",
    "def bm25_retrieve(query, bm25, tokenized_docs, top_k=5):\n",
    "    tokenized_query = query.split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "    return top_k_indices, scores[top_k_indices]\n",
    "\n",
    "# Dense Retrieval\n",
    "def dense_retrieve(query_embedding, doc_embeddings, top_k=5):\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return top_k_indices, similarities[top_k_indices]\n",
    "# Rank Fusion Retrieval\n",
    "def fusion_retrieve(dense_query_embedding, doc_embeddings, query, top_k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Implementa il rank fusion riutilizzando le funzioni esistenti di retrieval\n",
    "    \"\"\"\n",
    "    sparse_indices, sparse_scores = bm25_retrieve(query, bm25, tokenized_docs, top_k=len(doc_embeddings))\n",
    "    dense_indices, dense_scores = dense_retrieve(dense_query_embedding, doc_embeddings, top_k=len(doc_embeddings))\n",
    "    \n",
    "    all_sparse_scores = np.zeros(len(doc_embeddings))\n",
    "    all_dense_scores = np.zeros(len(doc_embeddings))\n",
    "    all_sparse_scores[sparse_indices] = sparse_scores\n",
    "    all_dense_scores[dense_indices] = dense_scores\n",
    "    \n",
    "    # Normalizza gli scores\n",
    "    all_sparse_scores = (all_sparse_scores - all_sparse_scores.min()) / (all_sparse_scores.max() - all_sparse_scores.min())\n",
    "    all_dense_scores = (all_dense_scores - all_dense_scores.min()) / (all_dense_scores.max() - all_dense_scores.min())\n",
    "    \n",
    "    # Combina i punteggi\n",
    "    combined_scores = alpha * all_dense_scores + (1 - alpha) * all_sparse_scores\n",
    "    \n",
    "    # Ottieni i top k risultati\n",
    "    top_k_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "    return top_k_indices, combined_scores[top_k_indices]\n",
    "\n",
    "# Cascading Retrieval\n",
    "def cascade_retrieve(dense_query_embedding, doc_embeddings, query, initial_k=100, final_k=5, dense_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Implementa il cascading retrieval riutilizzando le funzioni esistenti di retrieval\n",
    "    \"\"\"\n",
    "    # Stage 1: Usa BM25 per ottenere i candidati iniziali\n",
    "    initial_indices, _ = bm25_retrieve(query, bm25, tokenized_docs, top_k=initial_k)\n",
    "    \n",
    "    # Stage 2: Re-rank usando dense retrieval\n",
    "    candidate_embeddings = doc_embeddings[initial_indices]\n",
    "    _, dense_scores = dense_retrieve(dense_query_embedding, candidate_embeddings, top_k=len(initial_indices))\n",
    "    \n",
    "    # Filtra per threshold\n",
    "    qualified_mask = dense_scores >= dense_threshold\n",
    "    if np.sum(qualified_mask) >= final_k:\n",
    "        qualified_indices = np.where(qualified_mask)[0]\n",
    "        top_indices = qualified_indices[np.argsort(dense_scores[qualified_indices])[-final_k:][::-1]]\n",
    "    else:\n",
    "        top_indices = np.argsort(dense_scores)[-final_k:][::-1]\n",
    "    \n",
    "    # Mappa gli indici ai documenti originali\n",
    "    final_indices = initial_indices[top_indices]\n",
    "    final_scores = dense_scores[top_indices]\n",
    "    \n",
    "    return final_indices, final_scores\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Run retrieval experiments\n",
    "def run_retrieval_experiments():\n",
    "    results = {\n",
    "        \"sparse\": [],\n",
    "        \"dense\": [],\n",
    "        \"rank_fusion\": [],\n",
    "        \"cascade\": []\n",
    "    }\n",
    "    print(\"Running retrieval experiments on all queries...\")\n",
    "\n",
    "    for query, query_embedding in tqdm(zip(all_queries, query_embeddings), total=len(all_queries)):\n",
    "        query_text = query['title'] if isinstance(query, dict) else query\n",
    "        \n",
    "        # Sparse Retrieval (BM25)\n",
    "        sparse_indices, sparse_scores = bm25_retrieve(query_text, bm25, tokenized_docs)\n",
    "        sparse_results = [{\"doc_id\": all_docs[idx]['doc_id'], \"score\": float(score)} \n",
    "                         for idx, score in zip(sparse_indices, sparse_scores)]\n",
    "        results[\"sparse\"].append({\"query\": query, \"results\": sparse_results})\n",
    "\n",
    "        # Dense Retrieval (cosine similarity)\n",
    "        dense_indices, dense_scores = dense_retrieve(query_embedding, doc_embeddings)\n",
    "        dense_results = [{\"doc_id\": all_docs[idx]['doc_id'], \"score\": float(score)} \n",
    "                        for idx, score in zip(dense_indices, dense_scores)]\n",
    "        results[\"dense\"].append({\"query\": query, \"results\": dense_results})\n",
    "\n",
    "        # Rank Fusion\n",
    "        fusion_indices, fusion_scores = fusion_retrieve(\n",
    "            query_embedding, doc_embeddings, query_text\n",
    "        )\n",
    "        fusion_results = [{\"doc_id\": all_docs[idx]['doc_id'], \"score\": float(score)} \n",
    "                         for idx, score in zip(fusion_indices, fusion_scores)]\n",
    "        results[\"rank_fusion\"].append({\"query\": query, \"results\": fusion_results})\n",
    "\n",
    "        # Cascade Retrieval\n",
    "        cascade_indices, cascade_scores = cascade_retrieve(\n",
    "            query_embedding, doc_embeddings, query_text\n",
    "        )\n",
    "        cascade_results = [{\"doc_id\": all_docs[idx]['doc_id'], \"score\": float(score)} \n",
    "                          for idx, score in zip(cascade_indices, cascade_scores)]\n",
    "        results[\"cascade\"].append({\"query\": query, \"results\": cascade_results})\n",
    "\n",
    "    print(\"Saving results...\")\n",
    "    with open(\"retrieval_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(\"Retrieval results saved to retrieval_results.json\")\n",
    "    \n",
    "    return results\n",
    "run_retrieval_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SECTION 4: QA WITH LANGUAGE MODEL\n",
    "# QA for the first query\n",
    "QUERY_INDEX = 3\n",
    "query = all_queries[QUERY_INDEX - 1]\n",
    "query_text = query['title'] if isinstance(query, dict) else query\n",
    "\n",
    "# Retrieval calls\n",
    "dense_top_k_indices, dense_top_k_scores = dense_retrieve(query_embeddings[QUERY_INDEX], doc_embeddings)\n",
    "rank_top_k_indices, rank_top_k_scores = fusion_retrieve(\n",
    "    query_embeddings[QUERY_INDEX], \n",
    "    doc_embeddings, \n",
    "    query_text\n",
    ")\n",
    "cascading_top_k_indices, cascading_top_k_scores = cascade_retrieve(\n",
    "    query_embeddings[QUERY_INDEX], \n",
    "    doc_embeddings, \n",
    "    query_text\n",
    ")\n",
    "\n",
    "# Get retrieved documents\n",
    "dense_retrieved_docs = [all_docs[idx]['abstract'] for idx in dense_top_k_indices]\n",
    "rank_retrieved_docs = [all_docs[idx]['abstract'] for idx in rank_top_k_indices]\n",
    "cascading_retrieved_docs = [all_docs[idx]['abstract'] for idx in cascading_top_k_indices]\n",
    "\n",
    "# --- QUESTION-ANSWER USING DENSE \n",
    "context = \"\\n\".join(dense_retrieved_docs)\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "#prompt = f\"Question:\\n{query}\\n\\nAnswer:\"\n",
    "print(f\"Length of the prompt: {len(prompt.split())} words\")\n",
    "\n",
    "# Generate response\n",
    "lm_pipeline = pipeline(\"text-generation\", \n",
    "                      model=\"meta-llama/Llama-3.2-1B\",\n",
    "                      device=0 if device == \"cuda\" else -1)\n",
    "response = lm_pipeline(prompt, \n",
    "                      max_new_tokens=150, \n",
    "                      temperature=0.1, \n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "print(\"Generated Response:\")\n",
    "print(response)\n",
    "\n",
    "# ---- QUESTION-ANSWER WITH NO CONTEXT PROVIDED WITH RAG----\n",
    "prompt = f\"\"\"\n",
    "Question:\\n{query}\\n\\nAnswer:\n",
    "\"\"\"\n",
    "#print(f\"Prompt: {prompt}\")\n",
    "response = lm_pipeline(prompt,\n",
    "                      max_new_tokens=150,\n",
    "                      temperature=0.1,\n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "print(\"\\n\\n NO CONTEXT\")\n",
    "response = response.split(\"Answer:\")[1].strip()\n",
    "print(response)\n",
    "print(\"fine\")\n",
    "\n",
    "# ---- QUESTION-ANSWER USING RANK FUSION ----\n",
    "print(\"\\n\\n RANK FUSION CONTEXT\")\n",
    "context = \"\\n\".join(rank_retrieved_docs)\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query['title']}\\n\"\n",
    "print(f\"Length of the prompt: {len(prompt.split())} words\")\n",
    "\n",
    "# Generate response\n",
    "response = lm_pipeline(prompt, \n",
    "                      max_new_tokens=150, \n",
    "                      temperature=0.1, \n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "print(\"Generated Response:\")\n",
    "print(response)\n",
    "print(\"fine\")\n",
    "\n",
    "# ---- QUESTION-ANSWER USING CASCADING RETRIEVAL ----\n",
    "print(\"\\n\\n Generating response with cascading context\")\n",
    "context = \"\\n\".join(cascading_retrieved_docs)\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "print(f\"Length of the prompt: {len(prompt.split())} words\")\n",
    "\n",
    "# Generate response\n",
    "response = lm_pipeline(prompt, \n",
    "                      max_new_tokens=150, \n",
    "                      temperature=0.7, \n",
    "                      truncation=False)[0][\"generated_text\"]\n",
    "print(\"Generated Response:\")\n",
    "print(response)\n",
    "print(\"fine\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
