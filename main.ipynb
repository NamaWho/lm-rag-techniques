{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "RJLZFE1t2k9r",
      "metadata": {
        "id": "RJLZFE1t2k9r"
      },
      "source": [
        "# Project: Question-Answering using Retrieval Augmented Generation\n",
        "by L.Arduini, D.N.Ghaneh, L.Menchini, C.Petruzzella\n",
        "\n",
        "## Description\n",
        "This project implements a QA chatbot leveraging language models hosted on a scalable server infrastructure. It provides embeddings to facilitate query-answering capabilities with advanced retrieval mechanisms.\n",
        "\n",
        "## Instructions to Run\n",
        "\n",
        "### Prerequisites\n",
        "1. Python 3.10 or above.\n",
        "2. Access to a runtime environment with GPU support (e.g., NVIDIA T4 on Google Colab) for optimal performance.\n",
        "\n",
        "### Running the project\n",
        "- Switch the runtime to GPU (e.g., NVIDIA T4) for enhanced performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Av24RrStoD2G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av24RrStoD2G",
        "outputId": "e6bc019d-ade5-4aec-a330-c5057c94b092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ir_datasets in /usr/local/lib/python3.10/dist-packages (0.5.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.12.3)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (2.5.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.67.1)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.3.3)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (3.3.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (0.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (2024.12.14)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\n",
            "Requirement already satisfied: pytrec_eval in /usr/local/lib/python3.10/dist-packages (0.5)\n",
            "Requirement already satisfied: PyStemmer in /usr/local/lib/python3.10/dist-packages (2.2.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install ir_datasets\n",
        "!pip install rank_bm25\n",
        "!pip install sentence_transformers\n",
        "!pip install pytrec_eval\n",
        "!pip install PyStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uHAUTJ99oCgI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHAUTJ99oCgI",
        "outputId": "09abe522-74cc-4e61-eaf5-fae92ab18b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "CUDA Version: 12.1\n",
            "CUDA Cores: 40\n",
            "Total Memory: 15.84 GB\n",
            "Compute Capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "import ir_datasets\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import pytrec_eval\n",
        "import collections\n",
        "import itertools\n",
        "import heapq\n",
        "\n",
        "api_key = \"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\"\n",
        "login(token=api_key)\n",
        "\n",
        "# Check GPU availability\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        cuda_version = torch.version.cuda  # Retrieve CUDA version\n",
        "        gpu_properties = torch.cuda.get_device_properties(torch.cuda.current_device())\n",
        "        print(f\"Using GPU: {gpu_properties.name}\")\n",
        "        print(f\"CUDA Version: {cuda_version}\")\n",
        "        print(f\"CUDA Cores: {gpu_properties.multi_processor_count}\")\n",
        "        print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.2f} GB\")\n",
        "        print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "        print(\"Using MPS (Metal Performance Shaders)\")\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        print(\"Using CPU\")\n",
        "    return device\n",
        "\n",
        "device = get_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aafc8257",
      "metadata": {
        "id": "aafc8257"
      },
      "source": [
        "# Section 1: Dataset loading and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08dfaab3",
      "metadata": {
        "id": "08dfaab3"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "import re\n",
        "import string\n",
        "import Stemmer\n",
        "import nltk\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "# ------- Pre Initialization -------\n",
        "# 1. Compile regex patterns once globally\n",
        "# 2. Preload stopwords set\n",
        "# 3. Initialize stemmer\n",
        "\n",
        "ACRONYM_REGEX = re.compile(r\"(?<!\\w)\\.(?!\\d)\")\n",
        "PUNCTUATION_TRANS = str.maketrans(\"\", \"\", string.punctuation)\n",
        "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
        "STEMMER = Stemmer.Stemmer('english')\n",
        "\n",
        "# Define a cached function to stem individual words\n",
        "@lru_cache(maxsize=1000)\n",
        "def stem(word):\n",
        "    return STEMMER.stemWord(word)\n",
        "\n",
        "# ----------------------------------\n",
        "\n",
        "def preprocess(s):\n",
        "    \"\"\"\n",
        "    Preprocess a string for indexing or querying.\n",
        "\n",
        "    Args:\n",
        "        s: The input string.\n",
        "\n",
        "    Returns:\n",
        "        A list of preprocessed tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    s = s.lower()\n",
        "    s = s.replace(\"&\", \" and \")\n",
        "    # normalize quotes and dashes\n",
        "    s = s.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))\n",
        "    # remove unnecessary dots in acronyms (but not decimals)\n",
        "    s = ACRONYM_REGEX.sub(\"\", s)\n",
        "    # remove punctuation\n",
        "    s = s.translate(PUNCTUATION_TRANS)\n",
        "    # strip and remove extra spaces\n",
        "    s = \" \".join(s.split())\n",
        "\n",
        "    tokens = s.split()\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
        "    tokens = STEMMER.stemWords(tokens)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "print(\"Loading the trec covid dataset...\")\n",
        "dataset = ir_datasets.load(\"cord19/trec-covid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niSTrLIAjZno",
        "outputId": "33e31709-aa99-4d99-9c3a-2cc113ee19a3"
      },
      "id": "niSTrLIAjZno",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the trec covid dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ir_datasets import load\n",
        "from ir_datasets.formats import BaseDocs\n",
        "\n",
        "class FilteredDataset(BaseDocs):\n",
        "    def __init__(self, original_dataset, filtered_docs):\n",
        "        self.original_dataset = original_dataset\n",
        "        self.filtered_docs = filtered_docs\n",
        "\n",
        "    def docs_iter(self):\n",
        "        return iter(self.filtered_docs)\n",
        "\n",
        "    def docs_count(self):\n",
        "        return len(self.filtered_docs)\n",
        "\n",
        "def count_duplicates_by_abstract(dataset):\n",
        "    \"\"\"\n",
        "    Conta il numero di documenti con un abstract duplicato nel dataset.\n",
        "\n",
        "    :param dataset: Dataset ir_datasets (es. 'cord19/trec-covid').\n",
        "    :return: Il numero di documenti con un abstract duplicato.\n",
        "    \"\"\"\n",
        "    seen_abstracts = set()\n",
        "    duplicate_count = 0\n",
        "\n",
        "    for document in dataset.docs_iter():\n",
        "      print(document.abstract)\n",
        "      abstract = document.abstract  # Accedi al campo 'abstract' del documento\n",
        "      if abstract in seen_abstracts:\n",
        "        duplicate_count += 1\n",
        "      else:\n",
        "        seen_abstracts.add(abstract)\n",
        "\n",
        "    return duplicate_count\n",
        "\n",
        "def remove_duplicates_by_abstract(dataset):\n",
        "    \"\"\"\n",
        "    Rimuove i duplicati dal dataset basandosi sul campo 'abstract'.\n",
        "\n",
        "    :param dataset: Dataset ir_datasets (es. 'cord19/trec-covid').\n",
        "    :return: Un dataset senza duplicati nel campo 'abstract'.\n",
        "    \"\"\"\n",
        "    seen_abstracts = set()\n",
        "    unique_documents = []\n",
        "\n",
        "    for document in dataset.docs_iter():\n",
        "        abstract = document.abstract  # Accedi al campo 'abstract' del documento\n",
        "        if abstract not in seen_abstracts:\n",
        "            seen_abstracts.add(abstract)\n",
        "            unique_documents.append(document)\n",
        "\n",
        "    return FilteredDataset(dataset, unique_documents)\n",
        "\n",
        "duplicate_count = count_duplicates_by_abstract(dataset)\n",
        "total_documents = sum(1 for _ in dataset.docs_iter())  # Conta i documenti totali\n",
        "\n",
        "print(f\"Totale documenti nel dataset: {total_documents}\")\n",
        "print(f\"Numero di documenti duplicati: {duplicate_count}\")\n",
        "print(f\"Numero di documenti unici: {total_documents - duplicate_count}\")\n",
        "print(f\"Percentuale di documenti duplicati: {duplicate_count / total_documents * 100:.2f}%\")\n",
        "\n",
        "print(type(dataset))\n",
        "dataset = remove_duplicates_by_abstract(dataset)\n",
        "print(type(dataset))\n",
        "print(f\"Numero di documenti nel dataset pulito: {sum(1 for _ in dataset.docs_iter())}\")"
      ],
      "metadata": {
        "id": "S5uZYdrVixIF"
      },
      "id": "S5uZYdrVixIF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "393aded9-d0ac-45b7-ae2d-b4783fc021c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "393aded9-d0ac-45b7-ae2d-b4783fc021c1",
        "outputId": "899ada25-88cd-48a3-9d77-66f64a348c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing documents and queries...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "queries_iter",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-190baac42396>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mall_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueries_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mquery_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Title: {query.title}\\nDescription: {query.description}\\nNarrative: {query.narrative}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mall_queries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ir_datasets/formats/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Return method bound to this instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXTENSIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdocs_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: queries_iter"
          ]
        }
      ],
      "source": [
        "# Prepare documents and queries\n",
        "print(\"Preparing documents and queries...\")\n",
        "\n",
        "# put all documents and queries in a list of dictionaries\n",
        "all_docs = []\n",
        "for doc in dataset.docs_iter():\n",
        "    if doc.abstract:  # Controlla se default_text è presente\n",
        "        abstract = f\"Title: {doc.title} Text: {doc.abstract}\"\n",
        "    else:\n",
        "        abstract = f\"Title: {doc.title}\"  # Usa solo il titolo se il testo non è disponibile\n",
        "    all_docs.append({\"doc_id\": doc.doc_id, \"abstract\": abstract})\n",
        "\n",
        "all_queries = []\n",
        "for query in dataset.queries_iter():\n",
        "    query_text = f\"Title: {query.title}\\nDescription: {query.description}\\nNarrative: {query.narrative}\"\n",
        "    all_queries.append({\"query_id\": query.query_id, \"title\": query_text})\n",
        "\n",
        "# all_docs = [{\"doc_id\": doc.doc_id, \"abstract\": doc.title + \" \" + doc.default_text()} for doc in dataset.docs_iter()]\n",
        "# all_queries = [{\"query_id\": query.query_id, \"title\": query.title + \" \" + query.description + \" \" + query.narrative} for query in dataset.queries_iter()]\n",
        "\n",
        "# Print dataset size information\n",
        "print(f\"Summary: {len(all_docs)} documents and {len(all_queries)} queries are available in the dataset.\")\n",
        "\n",
        "# Tokenize documents\n",
        "tokenized_docs = [preprocess(doc) for doc in [docs[\"abstract\"] for docs in all_docs]]\n",
        "tokenized_queries = [preprocess(query) for query in [queries[\"title\"] for queries in all_queries]]\n",
        "print(\"Tokenization of documents is done.\")\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zdvajGS135b2",
      "metadata": {
        "id": "zdvajGS135b2"
      },
      "outputs": [],
      "source": [
        "# convert qrels to a dictionary\n",
        "qrels_dict = collections.defaultdict(dict)\n",
        "for qrel in dataset.qrels_iter():\n",
        "    qrels_dict[qrel.query_id][qrel.doc_id] = int(qrel.relevance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f863229",
      "metadata": {
        "id": "2f863229"
      },
      "source": [
        "# Section 2: Embeddings generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4acfb7-4dd6-41e4-a35f-a6d60d917f76",
      "metadata": {
        "id": "af4acfb7-4dd6-41e4-a35f-a6d60d917f76"
      },
      "outputs": [],
      "source": [
        "# Load or generate embeddings\n",
        "force_generate = False\n",
        "\n",
        "def generate_embeddings():\n",
        "    if not force_generate and os.path.exists(\"trec_covid_doc_embeddings.csv\") and os.path.exists(\"trec_covid_query_embeddings.csv\"):\n",
        "        print(\"Loading precomputed embeddings...\")\n",
        "        doc_embeddings = pd.read_csv(\"trec_covid_doc_embeddings.csv\").values\n",
        "        query_embeddings = pd.read_csv(\"trec_covid_query_embeddings.csv\").values\n",
        "    else:\n",
        "        print(\"No precomputed embeddings found.\")\n",
        "        print(\"Generating new embeddings using SentenceTransformer model 'sentence-transformers/all-MiniLM-L6-v2'.\")\n",
        "        model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
        "        doc_embeddings = model.encode([doc[\"abstract\"] for doc in all_docs], batch_size=32, show_progress_bar=True, normalize_embeddings=True)\n",
        "        query_embeddings = model.encode([query['title'] for query in all_queries], batch_size=32, show_progress_bar=True, normalize_embeddings=True)\n",
        "\n",
        "        # Save embeddings for future use\n",
        "        pd.DataFrame(doc_embeddings).to_csv(\"trec_covid_doc_embeddings.csv\", index=False)\n",
        "        pd.DataFrame(query_embeddings).to_csv(\"trec_covid_query_embeddings.csv\", index=False)\n",
        "\n",
        "    return doc_embeddings, query_embeddings\n",
        "\n",
        "doc_embeddings, query_embeddings = generate_embeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520c17ca",
      "metadata": {
        "id": "520c17ca"
      },
      "source": [
        "# Section 3: Retrieval implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bnATkuKEz2hJ",
      "metadata": {
        "id": "bnATkuKEz2hJ"
      },
      "outputs": [],
      "source": [
        "# Function to prepare run data for pytrec_eval\n",
        "def prepare_run_data(results):\n",
        "    \"\"\"\n",
        "    Prepares the run data in the format expected by pytrec_eval.\n",
        "    Converts numpy scores to native Python float for compatibility.\n",
        "    \"\"\"\n",
        "    run = {}\n",
        "    for query_results in results:\n",
        "        query_id = query_results['query']['query_id']\n",
        "        run[query_id] = {}\n",
        "        for doc_id, score in zip(query_results['results'], query_results['scores']):\n",
        "            run[query_id][doc_id] = float(score)  # Convert numpy type to float\n",
        "    return run"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99364747",
      "metadata": {
        "id": "99364747"
      },
      "source": [
        "### Document Retrieval Methods\n",
        "\n",
        "1. **BM25 Sparse Retrieval**:\n",
        "   - The **BM25 algorithm** is used to perform sparse retrieval on tokenized documents by calculating a relevance score for each document based on the query. It then returns the indices and relevance scores of the top-k most relevant documents.\n",
        "\n",
        "2. **Dense Retrieval**:\n",
        "   - **Dense retrieval** is performed by calculating the cosine similarity between the query embedding and the document embeddings. The top-k documents with the highest similarity scores are returned.\n",
        "\n",
        "3. **Rank Fusion Retrieval**:\n",
        "   - Results from both **BM25** and **dense retrieval** are combined using a **rank fusion** technique. Scores from both methods are normalized, weighted by a parameter `alpha`, and the top-k documents are returned based on the combined scores.\n",
        "\n",
        "4. **Cascading Retrieval**:\n",
        "   - Initially, a set of documents is retrieved using **BM25**. These documents are then re-ranked using dense retrieval, with a similarity threshold applied to filter documents. The top-k documents are returned based on the final ranking."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "cross_encoder_model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2').to(\"cuda\")\n",
        "cross_encoder_tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')"
      ],
      "metadata": {
        "id": "6raan8uKT6T8"
      },
      "id": "6raan8uKT6T8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5640a8c1-7740-4d63-8f45-0ecd4d816706",
      "metadata": {
        "id": "5640a8c1-7740-4d63-8f45-0ecd4d816706"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "# BM25 Sparse Retrieval\n",
        "def bm25_retrieve(query, bm25, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform sparse retrieval using BM25 on the tokenized documents.\n",
        "    Returns the indices and scores of the top-k documents.\n",
        "    \"\"\"\n",
        "    tokenized_query = preprocess(query)                                     # Tokenize the query into words\n",
        "    scores = bm25.get_scores(tokenized_query)                                   # Get BM25 scores for all documents\n",
        "    top_k_indices = np.argsort(scores)[-top_k:][::-1]                           # Get indices of top-k documents based on BM25 score\n",
        "    return top_k_indices, scores[top_k_indices]\n",
        "\n",
        "# Dense Retrieval\n",
        "def dense_retrieve(query_embedding, doc_embeddings, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform dense retrieval using cosine similarity between query and document embeddings.\n",
        "    Returns the indices and similarities of the top-k documents.\n",
        "    \"\"\"\n",
        "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]      # Compute cosine similarity\n",
        "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]                     # Get top-k indices based on similarity\n",
        "    return top_k_indices, similarities[top_k_indices]\n",
        "\n",
        "def combsum_fusion(dense_indices, dense_scores, sparse_indices, sparse_scores, top_k=5):\n",
        "    # Combine scores using CombSUM\n",
        "    all_doc_ids = np.concatenate((sparse_indices, dense_indices))\n",
        "    all_scores = np.concatenate((sparse_scores, dense_scores))\n",
        "    combined_scores = collections.defaultdict(float)\n",
        "    for doc_id, score in zip(all_doc_ids, all_scores):\n",
        "        combined_scores[doc_id] += score\n",
        "\n",
        "    # Retrieve top-k documents based on combined scores\n",
        "    top_docs = heapq.nlargest(top_k, combined_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "    # return top k indices and scores\n",
        "    return [doc[0] for doc in top_docs], [doc[1] for doc in top_docs]\n",
        "\n",
        "def neural_rerank(query_text, dense_indices, dense_scores, sparse_indices, sparse_scores, top_k=5):\n",
        "\n",
        "    doc_ids = np.concatenate((sparse_indices, dense_indices))\n",
        "    documents = []\n",
        "    for doc in doc_ids:\n",
        "        documents.append(all_docs[doc]['abstract'])\n",
        "    features = cross_encoder_tokenizer([query_text]*len(documents), documents, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "      scores = cross_encoder_model(**features).logits\n",
        "\n",
        "    # Rerank the documents by scores\n",
        "    doc_scores = {doc_id: score.item() for doc_id, score in zip(doc_ids, scores)}\n",
        "    reranked_doc_scores = dict(sorted(doc_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    # return top indices and top scores\n",
        "    return list(reranked_doc_scores.keys())[:top_k], list(reranked_doc_scores.values())[:top_k]\n",
        "\n",
        "# # Cascading Retrieval\n",
        "# def cascade_retrieve(dense_query_embedding, doc_embeddings, query, initial_k=1000, final_k=5, dense_threshold=0.7):\n",
        "#     \"\"\"\n",
        "#     Perform cascading retrieval: sparse retrieval followed by dense re-ranking.\n",
        "#     Filters documents based on a similarity threshold and returns the top-k results.\n",
        "#     \"\"\"\n",
        "#     # Stage 1: BM25 to get initial candidates\n",
        "#     initial_indices, _ = bm25_retrieve(query, bm25, top_k=initial_k)\n",
        "\n",
        "#     # Stage 2: Dense re-ranking of candidate documents\n",
        "#     candidate_embeddings = doc_embeddings[initial_indices]\n",
        "#     _, dense_scores = dense_retrieve(dense_query_embedding, candidate_embeddings, top_k=len(initial_indices))\n",
        "\n",
        "#     # Filter candidates by similarity threshold\n",
        "#     qualified_mask = dense_scores >= dense_threshold\n",
        "#     if np.sum(qualified_mask) >= final_k:\n",
        "#         # Select top-k qualified candidates\n",
        "#         qualified_indices = np.where(qualified_mask)[0]\n",
        "#         top_indices = qualified_indices[np.argsort(dense_scores[qualified_indices])[-final_k:][::-1]]\n",
        "#     else:\n",
        "#         # If there are not enough qualified candidates, select top-k by overall scores\n",
        "#         top_indices = np.argsort(dense_scores)[-final_k:][::-1]\n",
        "\n",
        "#     # Map filtered indices to original document IDs\n",
        "#     final_indices = initial_indices[top_indices]\n",
        "#     final_scores = dense_scores[top_indices]\n",
        "\n",
        "#     return final_indices, final_scores\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fcef89",
      "metadata": {
        "id": "80fcef89"
      },
      "source": [
        "This section of code performs several retrieval experiments using the four different Document Retrieval Methods described earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c417195d",
      "metadata": {
        "id": "c417195d"
      },
      "outputs": [],
      "source": [
        "# Run retrieval experiments\n",
        "def run_retrieval_experiments():\n",
        "    \"\"\"\n",
        "    Execute sparse, dense, rank fusion, and cascading retrieval for all queries.\n",
        "    Save the results to a JSON file for further analysis.\n",
        "    \"\"\"\n",
        "    results = {\"sparse\": [], \"dense\": [], \"rank_fusion\": [], \"cascade\": []}\n",
        "\n",
        "    print(\"Running retrieval experiments on all queries.\")\n",
        "\n",
        "    # Iterate over each query and its embedding\n",
        "    for query, query_embedding in tqdm(zip(all_queries, query_embeddings), total=len(all_queries)):\n",
        "        # Extract the query ID and text for the current query\n",
        "        query_id = query['query_id']\n",
        "        query_text = query['title']\n",
        "\n",
        "        # Sparse Retrieval using BM25\n",
        "        sparse_indices, sparse_scores = bm25_retrieve(query_text, bm25)                 # Retrieve the top-k BM25 documents and their scores\n",
        "        sparse_docs = [all_docs[idx]['doc_id'] for idx in sparse_indices]               # Get document IDs from the indices\n",
        "\n",
        "        # Dense Retrieval using cosine similarity\n",
        "        dense_indices, dense_scores = dense_retrieve(query_embedding, doc_embeddings)   # Retrieve the top-k documents based on cosine similarity of embeddings\n",
        "        dense_docs = [all_docs[idx]['doc_id'] for idx in dense_indices]\n",
        "\n",
        "        # Normalize scores\n",
        "        sparse_scores = zscore(sparse_scores)\n",
        "        dense_scores = zscore(dense_scores)\n",
        "        results[\"sparse\"].append({\"query\": query, \"results\": sparse_docs, \"scores\": sparse_scores}) # Store the BM25 results for the current query\n",
        "        results[\"dense\"].append({\"query\": query, \"results\": dense_docs, \"scores\": dense_scores})\n",
        "\n",
        "        # Rank Fusion Retrieval by combining sparse (BM25) and dense result\n",
        "        fusion_indices, fusion_scores = combsum_fusion(dense_indices, dense_scores, sparse_indices, sparse_scores)\n",
        "        fusion_docs = [all_docs[idx]['doc_id'] for idx in fusion_indices]\n",
        "        results[\"rank_fusion\"].append({\"query\": query, \"results\": fusion_docs, \"scores\": fusion_scores})\n",
        "\n",
        "        # Cascade Retrieval: compute sparse and dense retrieval, then use reranker\n",
        "        cascade_indices, cascade_scores = neural_rerank(query_text, dense_indices, dense_scores, sparse_indices, sparse_scores)\n",
        "        cascade_docs = [all_docs[idx]['doc_id'] for idx in cascade_indices]\n",
        "        results[\"cascade\"].append({\"query\": query, \"results\": cascade_docs, \"scores\": cascade_scores})\n",
        "    return results\n",
        "\n",
        "results = run_retrieval_experiments()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JOAqgAfe5W6H",
      "metadata": {
        "id": "JOAqgAfe5W6H"
      },
      "outputs": [],
      "source": [
        "run_sparse = prepare_run_data(results[\"sparse\"])\n",
        "run_dense = prepare_run_data(results[\"dense\"])\n",
        "run_rank_fusion = prepare_run_data(results[\"rank_fusion\"])\n",
        "run_cascade = prepare_run_data(results[\"cascade\"])\n",
        "\n",
        "# Evaluate results with pytrec_eval\n",
        "evaluator = pytrec_eval.RelevanceEvaluator(qrels_dict, {'recall.5', 'ndcg_cut.5'})\n",
        "eval_results_sparse = evaluator.evaluate(run_sparse)\n",
        "eval_results_dense = evaluator.evaluate(run_dense)\n",
        "eval_results_rank_fusion = evaluator.evaluate(run_rank_fusion)\n",
        "eval_results_cascade = evaluator.evaluate(run_cascade)\n",
        "\n",
        "# Aggregate metrics for overall performance\n",
        "aggregated_results = {\n",
        "    \"sparse\": {\n",
        "        metric: sum([res[metric] for res in eval_results_sparse.values()]) / len(eval_results_sparse)\n",
        "        for metric in eval_results_sparse[next(iter(eval_results_sparse))]\n",
        "    },\n",
        "    \"dense\": {\n",
        "        metric: sum([res[metric] for res in eval_results_dense.values()]) / len(eval_results_dense)\n",
        "        for metric in eval_results_dense[next(iter(eval_results_dense))]\n",
        "    },\n",
        "    \"rank_fusion\": {\n",
        "        metric: sum([res[metric] for res in eval_results_rank_fusion.values()]) / len(eval_results_rank_fusion)\n",
        "        for metric in eval_results_rank_fusion[next(iter(eval_results_rank_fusion))]\n",
        "    },\n",
        "    \"cascade\": {\n",
        "        metric: sum([res[metric] for res in eval_results_cascade.values()]) / len(eval_results_cascade)\n",
        "        for metric in eval_results_cascade[next(iter(eval_results_cascade))]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Aggregated results:\", json.dumps(aggregated_results, indent=4))\n",
        "print(\"Retrieval results and metrics saved to files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "548ac9dd",
      "metadata": {
        "id": "548ac9dd"
      },
      "source": [
        "# Section 4: QA with Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee13b08-c67c-49f4-92cc-35c9ab5eaf5e",
      "metadata": {
        "id": "cee13b08-c67c-49f4-92cc-35c9ab5eaf5e"
      },
      "outputs": [],
      "source": [
        "# QA for the first query\n",
        "QUERY_INDEX = 3                                                     # Index of the query to be used for retrieval\n",
        "query = all_queries[QUERY_INDEX - 1]                                # Select the query from the list based on the index\n",
        "query_text = query['title'] if isinstance(query, dict) else query   # Get the query text\n",
        "\n",
        "\n",
        "# Perform dense retrieval using query embedding and document embeddings\n",
        "dense_top_k_indices, dense_top_k_scores = dense_retrieve(query_embeddings[QUERY_INDEX], doc_embeddings)\n",
        "# Perform sparse retrieval using BM25 on the query text\n",
        "sparse_top_k_indices, sparse_top_k_scores = bm25_retrieve(query_text, bm25)\n",
        "# Perform rank fusion retrieval by combining BM25 and dense retrieval results\n",
        "rank_top_k_indices, rank_top_k_scores = combsum_fusion(dense_top_k_indices, dense_top_k_scores, sparse_top_k_indices, sparse_top_k_scores)\n",
        "# Perform cascading retrieval: first BM25, then re-rank with dense retrieval\n",
        "cascading_top_k_indices, cascading_top_k_scores = neural_rerank(query_text, dense_top_k_indices, dense_top_k_scores, sparse_top_k_indices, sparse_top_k_scores)\n",
        "\n",
        "# Get retrieved documents for each method\n",
        "dense_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(dense_top_k_indices)]\n",
        "sparse_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(sparse_top_k_indices)]\n",
        "rank_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(rank_top_k_indices)]\n",
        "cascading_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(cascading_top_k_indices)]\n",
        "\n",
        "# Definition of the model that will be used to generate the various responses.\n",
        "lm_pipeline = pipeline(\"text-generation\",\n",
        "                      model=\"meta-llama/Llama-3.2-1B\",\n",
        "                      device=0 if device == \"cuda\" else -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30deea44",
      "metadata": {
        "id": "30deea44"
      },
      "source": [
        "#### Question-answering using DENSE RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54044429-a3c4-4f7b-8a5c-7baef2f13f3b",
      "metadata": {
        "id": "54044429-a3c4-4f7b-8a5c-7baef2f13f3b"
      },
      "outputs": [],
      "source": [
        "print(\"------------------ DENSE RETRIEVAL ----------------------\\n\")\n",
        "context = \"\\n\".join(dense_retrieved_docs)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "# Generate response\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b79a94b3",
      "metadata": {
        "id": "b79a94b3"
      },
      "source": [
        "#### Question-answering using SPARSE RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4220ff7d-8bee-48d8-ae1e-31547e158ebf",
      "metadata": {
        "id": "4220ff7d-8bee-48d8-ae1e-31547e158ebf"
      },
      "outputs": [],
      "source": [
        "print(\"------------------ SPARSE RETRIEVAL ----------------------\\n\")\n",
        "context = \"\\n\".join(sparse_retrieved_docs)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "# Generate response\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d52aca9e",
      "metadata": {
        "id": "d52aca9e"
      },
      "source": [
        "#### Question-answering using RANK FUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c3b1e8-d6b2-45df-84d0-e36413b0f6f8",
      "metadata": {
        "id": "51c3b1e8-d6b2-45df-84d0-e36413b0f6f8"
      },
      "outputs": [],
      "source": [
        "print(\"------------------ RANK FUSION ----------------------\\n\")\n",
        "context = \"\\n\".join(rank_retrieved_docs)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "# Generate response\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69d431c",
      "metadata": {
        "id": "e69d431c"
      },
      "source": [
        "#### Question-answering using CASCADING RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b0c87d-9e2f-4429-9a57-74d60f720fd1",
      "metadata": {
        "id": "d4b0c87d-9e2f-4429-9a57-74d60f720fd1"
      },
      "outputs": [],
      "source": [
        "print(\"------------------ CASCADING RETRIEVAL ----------------------\\n\")\n",
        "context = \"\\n\".join(cascading_retrieved_docs)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "# Generate response\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8666f76",
      "metadata": {
        "id": "f8666f76"
      },
      "source": [
        "#### Question-answering WITH NO CONTEXT PROVIDED WITH RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6793863-6d60-46c2-983a-2d8d44fa15aa",
      "metadata": {
        "id": "d6793863-6d60-46c2-983a-2d8d44fa15aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"------------------ RESPONSE WITHOUT RAG ----------------------\\n\")\n",
        "prompt = f\"\"\"Question:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\"\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c28e3f7",
      "metadata": {
        "id": "6c28e3f7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "picked_queries = random.sample(all_queries, 5)\n",
        "\n",
        "for q in picked_queries:\n",
        "\n",
        "    # For each query, retrieve and rank documents independently\n",
        "    query_text = q['title']\n",
        "    cascading_top_k_indices, cascading_top_k_scores = neural_rerank(query_text, dense_top_k_indices, dense_top_k_scores, sparse_top_k_indices, sparse_top_k_scores)\n",
        "\n",
        "    # Use the top-k documents for that specific query\n",
        "    cascading_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(cascading_top_k_indices)]\n",
        "    cascading_context = \"\\n\".join(cascading_retrieved_docs)\n",
        "\n",
        "    # Repeat the process for rank fusion\n",
        "    rank_top_k_indices, rank_top_k_scores = combsum_fusion(dense_top_k_indices, dense_top_k_scores, sparse_top_k_indices, sparse_top_k_scores)\n",
        "    rank_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(rank_top_k_indices)]\n",
        "    rank_fusion_context = \"\\n\".join(rank_retrieved_docs)\n",
        "\n",
        "    cascading_prompt = f\"Context:\\n{cascading_context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "    rank_fusion_prompt = f\"Context:\\n{rank_fusion_context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "    # Generate response using language model\n",
        "    cascading_response = lm_pipeline(cascading_prompt,\n",
        "                           max_new_tokens=150,\n",
        "                           temperature=0.7,\n",
        "                           truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "    rank_fusion_response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.1,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "    # Extract the answer from the response\n",
        "    cascading_response = cascading_response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "    rank_fusion_response = rank_fusion_response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"\\nQuery: {query_text}\")\n",
        "    print(f\"Cascading Response: {cascading_response}\")\n",
        "    print(f\"Rank Fusion Response: {rank_fusion_response}\")\n",
        "    print(\"------------------------------\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}