{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "RJLZFE1t2k9r",
      "metadata": {
        "id": "RJLZFE1t2k9r"
      },
      "source": [
        "# Project: Question-Answering using Retrieval Augmented Generation\n",
        "by L.Arduini, D.N.Ghaneh, L.Menchini, C.Petruzzella\n",
        "\n",
        "## Description\n",
        "This project implements a QA chatbot leveraging language models hosted on a scalable server infrastructure. It provides embeddings to facilitate query-answering capabilities with advanced retrieval mechanisms.\n",
        "\n",
        "## Instructions to Run\n",
        "\n",
        "### Prerequisites\n",
        "1. Python 3.10 or above.\n",
        "2. Access to a runtime environment with GPU support (e.g., NVIDIA T4 on Google Colab) for optimal performance.\n",
        "\n",
        "### Running the project\n",
        "- Switch the runtime to GPU (e.g., NVIDIA T4) for enhanced performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "Av24RrStoD2G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av24RrStoD2G",
        "outputId": "fb05355e-f77e-4389-b51d-1c0c3e06911e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ir_datasets in /usr/local/lib/python3.10/dist-packages (0.5.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.12.3)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (2.5.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.67.1)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (4.3.3)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (3.3.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir_datasets) (0.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->ir_datasets) (2024.12.14)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\n",
            "Requirement already satisfied: pytrec_eval in /usr/local/lib/python3.10/dist-packages (0.5)\n",
            "Requirement already satisfied: PyStemmer in /usr/local/lib/python3.10/dist-packages (2.2.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install ir_datasets\n",
        "!pip install rank_bm25\n",
        "!pip install sentence_transformers\n",
        "!pip install pytrec_eval\n",
        "!pip install PyStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "uHAUTJ99oCgI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHAUTJ99oCgI",
        "outputId": "c255c9c9-86f5-437f-ea19-5ccddde03144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA Cores: 108\n",
            "Total Memory: 42.48 GB\n",
            "Compute Capability: 8.0\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "import ir_datasets\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import pytrec_eval\n",
        "import collections\n",
        "import itertools\n",
        "import heapq\n",
        "\n",
        "api_key = \"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\"\n",
        "login(token=api_key)\n",
        "\n",
        "# Check GPU availability\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        gpu_properties = torch.cuda.get_device_properties(torch.cuda.current_device())\n",
        "        print(f\"Using GPU: {gpu_properties.name}\")\n",
        "        print(f\"CUDA Cores: {gpu_properties.multi_processor_count}\")\n",
        "        print(f\"Total Memory: {gpu_properties.total_memory / 1e9:.2f} GB\")\n",
        "        print(f\"Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "        print(\"Using MPS (Metal Performance Shaders)\")\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        print(\"Using CPU\")\n",
        "    return device\n",
        "\n",
        "device = get_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aafc8257",
      "metadata": {
        "id": "aafc8257"
      },
      "source": [
        "# Section 1: Dataset loading and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "08dfaab3",
      "metadata": {
        "id": "08dfaab3"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "import re\n",
        "import string\n",
        "import Stemmer\n",
        "import nltk\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "# ------- Pre Initialization -------\n",
        "# 1. Compile regex patterns once globally\n",
        "# 2. Preload stopwords set\n",
        "# 3. Initialize stemmer\n",
        "\n",
        "ACRONYM_REGEX = re.compile(r\"(?<!\\w)\\.(?!\\d)\")\n",
        "PUNCTUATION_TRANS = str.maketrans(\"\", \"\", string.punctuation)\n",
        "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
        "STEMMER = Stemmer.Stemmer('english')\n",
        "\n",
        "# Define a cached function to stem individual words\n",
        "@lru_cache(maxsize=1000)\n",
        "def stem(word):\n",
        "    return STEMMER.stemWord(word)\n",
        "\n",
        "# ----------------------------------\n",
        "\n",
        "def preprocess(s):\n",
        "    \"\"\"\n",
        "    Preprocess a string for indexing or querying.\n",
        "\n",
        "    Args:\n",
        "        s: The input string.\n",
        "\n",
        "    Returns:\n",
        "        A list of preprocessed tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    s = s.lower()\n",
        "    s = s.replace(\"&\", \" and \")\n",
        "    # normalize quotes and dashes\n",
        "    s = s.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))\n",
        "    # remove unnecessary dots in acronyms (but not decimals)\n",
        "    s = ACRONYM_REGEX.sub(\"\", s)\n",
        "    # remove punctuation\n",
        "    s = s.translate(PUNCTUATION_TRANS)\n",
        "    # strip and remove extra spaces\n",
        "    s = \" \".join(s.split())\n",
        "\n",
        "    tokens = s.split()\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
        "    tokens = STEMMER.stemWords(tokens)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "393aded9-d0ac-45b7-ae2d-b4783fc021c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "393aded9-d0ac-45b7-ae2d-b4783fc021c1",
        "outputId": "d58e9b62-58f5-4703-9fc1-c01e94946c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the trec covid dataset...\n",
            "Preparing documents and queries...\n",
            "Summary: 192509 documents and 50 queries are available in the dataset.\n",
            "Tokenization of documents is done.\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "print(\"Loading the trec covid dataset...\")\n",
        "dataset = ir_datasets.load(\"cord19/trec-covid\")\n",
        "\n",
        "# Prepare documents and queries\n",
        "print(\"Preparing documents and queries...\")\n",
        "\n",
        "# put all documents and queries in a list of dictionaries\n",
        "all_docs = []\n",
        "for doc in dataset.docs_iter():\n",
        "    if doc.abstract:  # Controlla se default_text è presente\n",
        "        abstract = f\"Title: {doc.title} Text: {doc.abstract}\"\n",
        "    else:\n",
        "        abstract = f\"Title: {doc.title}\"  # Usa solo il titolo se il testo non è disponibile\n",
        "    all_docs.append({\"doc_id\": doc.doc_id, \"abstract\": abstract})\n",
        "\n",
        "all_queries = []\n",
        "for query in dataset.queries_iter():\n",
        "    query_text = f\"Title: {query.title}\\nDescription: {query.description}\\nNarrative: {query.narrative}\"\n",
        "    all_queries.append({\"query_id\": query.query_id, \"title\": query_text})\n",
        "\n",
        "# all_docs = [{\"doc_id\": doc.doc_id, \"abstract\": doc.title + \" \" + doc.default_text()} for doc in dataset.docs_iter()]\n",
        "# all_queries = [{\"query_id\": query.query_id, \"title\": query.title + \" \" + query.description + \" \" + query.narrative} for query in dataset.queries_iter()]\n",
        "\n",
        "# Print dataset size information\n",
        "print(f\"Summary: {len(all_docs)} documents and {len(all_queries)} queries are available in the dataset.\")\n",
        "\n",
        "# Tokenize documents\n",
        "tokenized_docs = [preprocess(doc) for doc in [docs[\"abstract\"] for docs in all_docs]]\n",
        "tokenized_queries = [preprocess(query) for query in [queries[\"title\"] for queries in all_queries]]\n",
        "print(\"Tokenization of documents is done.\")\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "zdvajGS135b2",
      "metadata": {
        "id": "zdvajGS135b2"
      },
      "outputs": [],
      "source": [
        "# convert qrels to a dictionary\n",
        "qrels_dict = collections.defaultdict(dict)\n",
        "for qrel in dataset.qrels_iter():\n",
        "    qrels_dict[qrel.query_id][qrel.doc_id] = int(qrel.relevance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f863229",
      "metadata": {
        "id": "2f863229"
      },
      "source": [
        "# Section 2: Embeddings generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "af4acfb7-4dd6-41e4-a35f-a6d60d917f76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af4acfb7-4dd6-41e4-a35f-a6d60d917f76",
        "outputId": "d6cd70d7-94ad-4e12-8f7c-5b72a98436e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading precomputed embeddings...\n"
          ]
        }
      ],
      "source": [
        "# Load or generate embeddings\n",
        "force_generate = False\n",
        "\n",
        "def generate_embeddings():\n",
        "    if not force_generate and os.path.exists(\"trec_covid_doc_embeddings.csv\") and os.path.exists(\"trec_covid_query_embeddings.csv\"):\n",
        "        print(\"Loading precomputed embeddings...\")\n",
        "        doc_embeddings = pd.read_csv(\"trec_covid_doc_embeddings.csv\").values\n",
        "        query_embeddings = pd.read_csv(\"trec_covid_query_embeddings.csv\").values\n",
        "    else:\n",
        "        print(\"No precomputed embeddings found.\")\n",
        "        print(\"Generating new embeddings using SentenceTransformer model 'sentence-transformers/all-MiniLM-L6-v2'.\")\n",
        "        model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
        "        doc_embeddings = model.encode([doc[\"abstract\"] for doc in all_docs], batch_size=32, show_progress_bar=True, normalize_embeddings=True)\n",
        "        query_embeddings = model.encode([query['title'] for query in all_queries], batch_size=32, show_progress_bar=True, normalize_embeddings=True)\n",
        "\n",
        "        # Save embeddings for future use\n",
        "        pd.DataFrame(doc_embeddings).to_csv(\"trec_covid_doc_embeddings.csv\", index=False)\n",
        "        pd.DataFrame(query_embeddings).to_csv(\"trec_covid_query_embeddings.csv\", index=False)\n",
        "\n",
        "    return doc_embeddings, query_embeddings\n",
        "\n",
        "doc_embeddings, query_embeddings = generate_embeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520c17ca",
      "metadata": {
        "id": "520c17ca"
      },
      "source": [
        "# Section 3: Retrieval implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "bnATkuKEz2hJ",
      "metadata": {
        "id": "bnATkuKEz2hJ"
      },
      "outputs": [],
      "source": [
        "# Function to prepare run data for pytrec_eval\n",
        "def prepare_run_data(results):\n",
        "    \"\"\"\n",
        "    Prepares the run data in the format expected by pytrec_eval.\n",
        "    Converts numpy scores to native Python float for compatibility.\n",
        "    \"\"\"\n",
        "    run = {}\n",
        "    for query_results in results:\n",
        "        query_id = query_results['query']['query_id']\n",
        "        run[query_id] = {}\n",
        "        for doc_id, score in zip(query_results['results'], query_results['scores']):\n",
        "            run[query_id][doc_id] = float(score)  # Convert numpy type to float\n",
        "    return run"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99364747",
      "metadata": {
        "id": "99364747"
      },
      "source": [
        "### Document Retrieval Methods\n",
        "\n",
        "1. **BM25 Sparse Retrieval**:\n",
        "   - The **BM25 algorithm** is used to perform sparse retrieval on tokenized documents by calculating a relevance score for each document based on the query. It then returns the indices and relevance scores of the top-k most relevant documents.\n",
        "\n",
        "2. **Dense Retrieval**:\n",
        "   - **Dense retrieval** is performed by calculating the cosine similarity between the query embedding and the document embeddings. The top-k documents with the highest similarity scores are returned.\n",
        "\n",
        "3. **Rank Fusion Retrieval**:\n",
        "   - Results from both **BM25** and **dense retrieval** are combined using a **rank fusion** technique. Scores from both methods are normalized, weighted by a parameter `alpha`, and the top-k documents are returned based on the combined scores.\n",
        "\n",
        "4. **Cascading Retrieval**:\n",
        "   - Initially, a set of documents is retrieved using **BM25**. These documents are then re-ranked using dense retrieval, with a similarity threshold applied to filter documents. The top-k documents are returned based on the final ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "5640a8c1-7740-4d63-8f45-0ecd4d816706",
      "metadata": {
        "id": "5640a8c1-7740-4d63-8f45-0ecd4d816706"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "# BM25 Sparse Retrieval\n",
        "def bm25_retrieve(query, bm25, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform sparse retrieval using BM25 on the tokenized documents.\n",
        "    Returns the indices and scores of the top-k documents.\n",
        "    \"\"\"\n",
        "    tokenized_query = preprocess(query)                                     # Tokenize the query into words\n",
        "    scores = bm25.get_scores(tokenized_query)                                   # Get BM25 scores for all documents\n",
        "    top_k_indices = np.argsort(scores)[-top_k:][::-1]                           # Get indices of top-k documents based on BM25 score\n",
        "    return top_k_indices, scores[top_k_indices]\n",
        "\n",
        "# Dense Retrieval\n",
        "def dense_retrieve(query_embedding, doc_embeddings, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform dense retrieval using cosine similarity between query and document embeddings.\n",
        "    Returns the indices and similarities of the top-k documents.\n",
        "    \"\"\"\n",
        "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]      # Compute cosine similarity\n",
        "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]                     # Get top-k indices based on similarity\n",
        "    return top_k_indices, similarities[top_k_indices]\n",
        "\n",
        "def combsum_fusion(query_id, dense_indices, dense_scores, sparse_indices, sparse_scores, top_k=5):\n",
        "    # Combine scores using CombSUM\n",
        "    all_doc_ids = np.concatenate((sparse_indices, dense_indices))\n",
        "    all_scores = np.concatenate((sparse_scores, dense_scores))\n",
        "    combined_scores = collections.defaultdict(float)\n",
        "    for doc_id, score in zip(all_doc_ids, all_scores):\n",
        "        combined_scores[doc_id] += score\n",
        "\n",
        "    # Retrieve top-k documents based on combined scores\n",
        "    top_docs = heapq.nlargest(top_k, combined_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "    # return top k indices and scores\n",
        "    return [doc[0] for doc in top_docs], [doc[1] for doc in top_docs]\n",
        "\n",
        "# Cascading Retrieval\n",
        "def cascade_retrieve(dense_query_embedding, doc_embeddings, query, initial_k=1000, final_k=5, dense_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Perform cascading retrieval: sparse retrieval followed by dense re-ranking.\n",
        "    Filters documents based on a similarity threshold and returns the top-k results.\n",
        "    \"\"\"\n",
        "    # Stage 1: BM25 to get initial candidates\n",
        "    initial_indices, _ = bm25_retrieve(query, bm25, top_k=initial_k)\n",
        "\n",
        "    # Stage 2: Dense re-ranking of candidate documents\n",
        "    candidate_embeddings = doc_embeddings[initial_indices]\n",
        "    _, dense_scores = dense_retrieve(dense_query_embedding, candidate_embeddings, top_k=len(initial_indices))\n",
        "\n",
        "    # Filter candidates by similarity threshold\n",
        "    qualified_mask = dense_scores >= dense_threshold\n",
        "    if np.sum(qualified_mask) >= final_k:\n",
        "        # Select top-k qualified candidates\n",
        "        qualified_indices = np.where(qualified_mask)[0]\n",
        "        top_indices = qualified_indices[np.argsort(dense_scores[qualified_indices])[-final_k:][::-1]]\n",
        "    else:\n",
        "        # If there are not enough qualified candidates, select top-k by overall scores\n",
        "        top_indices = np.argsort(dense_scores)[-final_k:][::-1]\n",
        "\n",
        "    # Map filtered indices to original document IDs\n",
        "    final_indices = initial_indices[top_indices]\n",
        "    final_scores = dense_scores[top_indices]\n",
        "\n",
        "    return final_indices, final_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fcef89",
      "metadata": {
        "id": "80fcef89"
      },
      "source": [
        "This section of code performs several retrieval experiments using the four different Document Retrieval Methods described earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "c417195d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c417195d",
        "outputId": "5d90d8a9-84ca-4949-a67b-60e53f221d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running retrieval experiments on all queries.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[171232  96819 113751 163782 118377] [ 83277 159595 162667 128664 169722]\n",
            "[83277, 171232, 96819, 113751, 159595] [1.8129179657310328, 1.0137063005060618, 1.0137063005060618, 0.322235573161346, 0.13075101228649974]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 1/50 [00:04<03:16,  4.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[190981  97988 114172 177067  74696] [124988 124987 124986 134539 176603]\n",
            "[190981, 97988, 124988, 124987, 124986] [1.5547674586356315, 0.8228845261916485, 0.6558325162425469, 0.6558325162425469, 0.6558325162425469]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 2/50 [00:09<03:50,  4.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[177580 188839 179261 135589  97732] [112009 112008  49085 119779 119778]\n",
            "[177580, 112009, 112008, 49085, 188839] [1.9831543001301857, 0.8164965809277084, 0.8164965809277084, 0.8164965809277084, -0.30636577030555073]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/50 [00:14<03:54,  4.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[108427 152540  88890 109489 109488] [175123 144067 119240  76337 120318]\n",
            "[175123, 108427, 152540, 144067, 119240] [1.7900161983757648, 1.2743885469333576, 1.1651004475402145, 0.049646853940860824, 0.049646853940860824]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/50 [00:17<03:13,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 75973  72253  80384 180990  81903] [110689 110688 149313 116335 116336]\n",
            "[75973, 110689, 110688, 149313, 72253] [1.7995728793199433, 0.8208789614537716, 0.8208789614537716, 0.8077034720081534, 0.3849732323515608]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 5/50 [00:23<03:40,  4.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 59511 182089  99662 109740 109741] [ 44665 107284 107285 162605  65060]\n",
            "[59511, 44665, 107284, 107285, 182089] [1.9139944011064942, 0.8842058667941491, 0.7216316462963016, 0.7216316462963016, 0.04088988274871325]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/50 [00:27<03:19,  4.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110864  47930 161541  90059  60591] [ 91397 168247 122213 192502  40838]\n",
            "[91397, 110864, 47930, 168247, 161541] [1.6086997448975642, 0.9209620646207547, 0.9209620646207547, 0.7464146356266144, -0.01220469298666555]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/50 [00:32<03:16,  4.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[128074  63907 166142 143437  74888] [169269 129379 129380  66985  44665]\n",
            "[128074, 63907, 169269, 129379, 129380] [1.1239399532772838, 1.1239399532772838, 0.8164718066790388, 0.8164718066790388, 0.8164718066790388]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 8/50 [00:37<03:15,  4.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 71741  74369 121096  57256  84566] [ 76782  76123 100066 138021 100067]\n",
            "[71741, 76782, 76123, 74369, 121096] [1.9597326237359007, 1.8401401883441382, 0.29859288002206485, -0.14489456562518147, -0.5227120551160076]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 9/50 [00:41<03:05,  4.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[134656  34042 132564  95734 132563] [142157 169075 123859  59878 100526]\n",
            "[134656, 142157, 169075, 34042, 132564] [1.9988883700709201, 1.7711228608763985, 0.45349006701449535, -0.4351662736821994, -0.521240698796237]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 10/50 [00:46<03:07,  4.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75701 83572 95117 95119 95118] [73594 75632 89152 69027 95164]\n",
            "[73594, 75701, 83572, 75632, 89152] [1.9982841843745727, 1.9928901147353248, -0.3350824702115195, -0.42577693333372524, -0.49692047021893104]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 11/50 [00:51<03:04,  4.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 69862 160538 130155 149344  75490] [ 91051  23699 133351  33969 111252]\n",
            "[69862, 91051, 23699, 133351, 160538] [1.3802204507736096, 0.9272699283576785, 0.7823023832882275, 0.7213229671566134, 0.2852890235972231]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 12/50 [00:55<02:54,  4.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[126533 126534  62463  76422  74777] [103710 103711  41389 112307 112306]\n",
            "[103710, 103711, 126533, 126534, 62463] [0.9480169138988198, 0.9480169138988198, 0.8152137481360494, 0.8152137481360494, 0.8152137481360494]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 13/50 [00:58<02:37,  4.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 63823 127975 127976 176578 124921] [ 63823 127976 127975 129850 152550]\n",
            "[63823, 127975, 127976, 129850, 152550] [1.7948523718574343, 1.5470390260012792, 1.5470390260012792, -1.2197203405383998, -1.2197203405383998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 14/50 [01:04<02:49,  4.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 15571 178906 133396 183338  89338] [133396 178906 157636  76516  94626]\n",
            "[178906, 133396, 15571, 157636, 183338] [2.1008752716389685, 1.6294043036535766, 0.9832292088669622, -0.6385290096354259, -0.657362905126163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 15/50 [01:10<02:54,  4.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 72624 109235 138731    686 157636] [157636  76516 177857 113048 130692]\n",
            "[72624, 76516, 109235, 138731, 686] [1.1985236403390078, 1.1264895405043687, 0.5705194383491641, 0.3180859109250407, -0.3511714353225218]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 16/50 [01:15<02:52,  5.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[113040 149126 132158  67758 132159] [174759 127553  90139  28617 132989]\n",
            "[113040, 149126, 174759, 127553, 90139] [1.2247448713915865, 1.2247448713915865, 0.9792394124626624, 0.9792387807760277, 0.08614091447308135]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 17/50 [01:19<02:38,  4.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[177453 187630 143501 109591  46796] [ 62094 126208  33196 138108  94878]\n",
            "[177453, 62094, 187630, 126208, 33196] [1.2387772982745087, 1.232156893697772, 1.2102247947488016, 0.5563778454750697, 0.2550490060862854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 18/50 [01:23<02:21,  4.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[125732 156229 130033 130032 130034] [132368 155496 130033 130032 130034]\n",
            "[125732, 132368, 155496, 156229, 130033] [1.6346947277173263, 1.2247448713915892, 1.2247448713915892, 0.7070266924879437, -1.597070387662817]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 19/50 [01:27<02:15,  4.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 85901 120340 121425 101155  82223] [103985 103986 107836 114930 146983]\n",
            "[85901, 120340, 103985, 103986, 107836] [1.182140109425385, 1.182140109425385, 1.0460385668495182, 1.0460385668495182, 0.18440491476653448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 20/50 [01:33<02:24,  4.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 70776  76194  71175 142676  96592] [ 70407 165621 108654 177176  76879]\n",
            "[70776, 70407, 165621, 108654, 76194] [1.869937733933687, 0.9501478792222919, 0.8751384394937671, 0.5839055640934511, 0.11341407371308973]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 21/50 [01:38<02:19,  4.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130470 162637 118401 184585 185788] [ 36686  59013  97524 114898 130470]\n",
            "[36686, 162637, 130470, 59013, 118401] [1.8217781988271178, 0.9662352639811456, 0.49544025721893425, 0.34039748254135127, -0.30879906269343205]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 22/50 [01:42<02:08,  4.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 94041 114966 140117 170641  60124] [ 29570  91109 131463 170030  36686]\n",
            "[29570, 94041, 114966, 140117, 91109] [1.0028532341290066, 0.8072778345821708, 0.8011142338124553, 0.7299063044625255, 0.33676923440406165]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 23/50 [01:46<02:02,  4.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101619  27442 186323 185135 108550] [190718  27442  60068 110788 138403]\n",
            "[101619, 27442, 190718, 186323, 60068] [1.3623517670399088, 1.2940888003593392, 1.19834213434471, 0.47762343665956786, 0.2818794592655626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 24/50 [01:51<01:57,  4.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  5631  42836 105270  46508 179657] [118251 146042 163360 119500 119501]\n",
            "[5631, 118251, 146042, 42836, 105270] [1.9999999999999982, 1.924355217675723, 0.04645591780023721, -0.500000000000002, -0.500000000000002]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 25/50 [01:56<01:57,  4.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[187272  95514 100801 100802 174212] [116018 116017  52673  70227 151132]\n",
            "[187272, 95514, 116018, 116017, 52673] [1.2245211381335213, 1.2245211381335213, 1.2195045278744927, 1.2195045278744927, -0.6442908607285626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 26/50 [01:59<01:45,  4.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 75875 175845 116010  74228 114754] [ 11977  44380 106970  38162 185762]\n",
            "[11977, 44380, 106970, 75875, 175845] [0.8154279363359512, 0.8154279363359512, 0.8154279363359512, 0.5643430044296882, 0.4781810342046631]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 27/50 [02:03<01:34,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 92481  30954  77095 177171  92104] [158910 107772 164717  48875  26496]\n",
            "[92481, 30954, 158910, 107772, 77095] [1.1582937864286302, 1.1582937864286302, 1.1399581570892465, 1.1399581570892465, -0.242601669446577]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 28/50 [02:07<01:29,  4.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 26598 143548 100797  97198 183094] [100572 191657  38761 121418 187114]\n",
            "[26598, 100572, 191657, 143548, 38761] [1.7295314746697688, 1.609406656180062, 0.5674449346415067, 0.5124562985140529, -0.14665086875039918]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 29/50 [02:14<01:46,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 41572 103912 132956 147251  59847] [69806 71887 29974 94833 91515]\n",
            "[69806, 41572, 103912, 132956, 71887] [1.99164629567943, 1.0915936835581659, 1.0915936835581659, -0.17823931470300783, -0.3426711979307875]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 30/50 [02:18<01:33,  4.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 87457  78609  40461    298 157269] [147518 125051 130235  65924 110819]\n",
            "[87457, 147518, 125051, 78609, 40461] [1.322214456043391, 1.2037373003553449, 1.1876903257727651, 0.6572618082001531, 0.28650354255793076]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 31/50 [02:23<01:31,  4.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  5546  48815  75945  24322 137522] [192411 159928  49085 112008 112009]\n",
            "[192411, 5546, 48815, 75945, 159928] [1.9525461931876926, 1.2600177207263905, 0.5543582161469982, 0.5504964362404515, -0.06880296598869275]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 32/50 [02:27<01:21,  4.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 77746 135188  34077   3979 185191] [ 50934 162538  65924 130235  99278]\n",
            "[77746, 50934, 162538, 135188, 34077] [1.7277074380935638, 1.644544087494254, 0.6615873866135493, 0.1764138543378974, 0.030518394336924257]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 33/50 [02:33<01:26,  5.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150097 114279 143566  70036 186301] [ 34803  96525  96526  96527 135588]\n",
            "[150097, 34803, 114279, 143566, 96525] [1.5916587823701784, 1.313803817631766, 0.26313359318993124, 0.17883200736264562, 0.15823849234517437]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 34/50 [02:38<01:19,  4.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 72705  90561 165927 125969 127078] [190724  89707  68382  40489 102714]\n",
            "[72705, 190724, 89707, 90561, 165927] [1.6354066611904399, 1.2848724145416028, 0.9632232719243583, 0.09844591423712706, 0.001762000746187557]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 35/50 [02:45<01:22,  5.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[188308  76309 158588 172170  77358] [192114  71753  72130  61143 168736]\n",
            "[192114, 188308, 76309, 71753, 72130] [1.9414402461510438, 1.553995950000475, 0.7100878856271458, -0.06087293591988388, -0.4514573222398731]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 36/50 [02:49<01:10,  5.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[113074 113073 174447  77505 181422] [ 95006  95007 150827 174447 113074]\n",
            "[95006, 95007, 113073, 150827, 174447] [0.8164969999987154, 0.8164969999987154, 0.8164965809277245, 0.8164957427854653, -0.40824803578462654]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 37/50 [02:53<01:03,  4.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[105443 150444 105442  54416 117989] [99620 37680 37103 98990 98989]\n",
            "[99620, 37680, 105443, 150444, 105442] [1.3646937846007896, 1.0643460092140429, 0.8164965809277219, 0.8164965809277219, 0.8164965809277219]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 38/50 [02:59<01:02,  5.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[102401  40247 102402  90260  90259] [ 97022 151462 129127 129126 105004]\n",
            "[97022, 102401, 40247, 102402, 151462] [1.8608103654448716, 1.2056330189154003, 0.5906485371328112, 0.5906485371328112, 0.03502892854250543]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 39/50 [03:04<00:57,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[105068 156067  73493  74351  75487] [145013 159928 135889 113728  73493]\n",
            "[145013, 105068, 156067, 159928, 135889] [1.435943462561242, 1.2126932594685793, 1.2126932594685793, 0.9351357951277997, -0.5643793948320014]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 40/50 [03:11<00:55,  5.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[168442 129328  43554 106082 106083] [152284 120975 120974 137575  46826]\n",
            "[168442, 129328, 152284, 120975, 120974] [1.2247448713915834, 1.2247448713915834, 0.8112569032937226, 0.8112569032937226, 0.8112569032937226]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 41/50 [03:19<00:56,  6.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[112397 141714  95384  95383   5648] [149566  75227 132762 154307 125491]\n",
            "[112397, 149566, 75227, 141714, 95384] [1.7955434769616214, 1.607160075115966, 0.6194742731181511, -0.16456950672530585, -0.16456950672530585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 42/50 [03:24<00:48,  6.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 95776  95775  34080 156839  68476] [95776 95775 34080 11699 93262]\n",
            "[95776, 95775, 34080, 11699, 156839] [1.5974007467598876, 1.5974007467598876, 1.5244678685927924, -0.5451751779886697, -1.1677951249643108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 43/50 [03:29<00:39,  5.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 14395  10357 147810  69510 143501] [102044 179146  97755 153645  56944]\n",
            "[102044, 14395, 10357, 179146, 97755] [1.7437237431109334, 1.5194372440269732, 0.8690593117845777, 0.11255838349288622, -0.20341142939484702]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 44/50 [03:34<00:32,  5.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 99116 188302  39265 101315 101316] [110477 177106  38652  76225 173818]\n",
            "[110477, 99116, 188302, 177106, 38652] [1.292017781039793, 1.2247448713915954, 1.2247448713915954, 0.5345508356387978, 0.2293754066168278]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 45/50 [03:39<00:26,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[174094  92766  92767  97922 178098] [184248 105178  42731  71503 118458]\n",
            "[184248, 174094, 92766, 92767, 105178] [1.860153008084353, 0.8164965809277144, 0.8164965809277144, 0.8164965809277144, 0.18846610211367104]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 46/50 [03:44<00:21,  5.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[117930 154212  68486 131698 172947] [177904  90261 157214 120130 120131]\n",
            "[177904, 90261, 117930, 154212, 68486] [1.224744871391592, 1.224744871391592, 1.0782584969615774, 1.0782584969615774, 0.14610775527313577]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 47/50 [03:50<00:16,  5.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 73761 119642  55962 119643  70016] [183284 113977  47725 110625 112979]\n",
            "[73761, 183284, 113977, 119642, 55962] [1.9687337102699853, 1.415885199816272, 0.9953079110686763, -0.37849178592281574, -0.37849178592281574]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 48/50 [03:57<00:12,  6.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 71709 140965 188043 134029 135964] [191751 114294 114293 190406 111696]\n",
            "[71709, 191751, 114294, 114293, 140965] [1.7156891364013245, 0.8164965809277234, 0.8164965809277234, 0.8164965809277234, 0.21851815852676962]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 49/50 [04:04<00:06,  6.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  3979   4490  69958   4743 160772] [ 90862 138356 140858 129323 129322]\n",
            "[3979, 90862, 138356, 4490, 69958] [1.4470102390603647, 1.22474487139159, 1.22474487139159, 0.48758019193991486, 0.327009760291939]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [04:09<00:00,  5.00s/it]\n"
          ]
        }
      ],
      "source": [
        "# Run retrieval experiments\n",
        "def run_retrieval_experiments():\n",
        "    \"\"\"\n",
        "    Execute sparse, dense, rank fusion, and cascading retrieval for all queries.\n",
        "    Save the results to a JSON file for further analysis.\n",
        "    \"\"\"\n",
        "    results = {\"sparse\": [], \"dense\": [], \"rank_fusion\": [], \"cascade\": []}\n",
        "\n",
        "    print(\"Running retrieval experiments on all queries.\")\n",
        "\n",
        "    # Iterate over each query and its embedding\n",
        "    for query, query_embedding in tqdm(zip(all_queries, query_embeddings), total=len(all_queries)):\n",
        "        # Extract the query ID and text for the current query\n",
        "        query_id = query['query_id']\n",
        "        query_text = query['title']\n",
        "\n",
        "        # Sparse Retrieval using BM25\n",
        "        sparse_indices, sparse_scores = bm25_retrieve(query_text, bm25)                 # Retrieve the top-k BM25 documents and their scores\n",
        "        sparse_docs = [all_docs[idx]['doc_id'] for idx in sparse_indices]               # Get document IDs from the indices\n",
        "\n",
        "        # Dense Retrieval using cosine similarity\n",
        "        dense_indices, dense_scores = dense_retrieve(query_embedding, doc_embeddings)   # Retrieve the top-k documents based on cosine similarity of embeddings\n",
        "        dense_docs = [all_docs[idx]['doc_id'] for idx in dense_indices]\n",
        "\n",
        "        # Normalize scores\n",
        "        sparse_scores = zscore(sparse_scores)\n",
        "        dense_scores = zscore(dense_scores)\n",
        "        results[\"sparse\"].append({\"query\": query, \"results\": sparse_docs, \"scores\": sparse_scores}) # Store the BM25 results for the current query\n",
        "        results[\"dense\"].append({\"query\": query, \"results\": dense_docs, \"scores\": dense_scores})\n",
        "\n",
        "        print(sparse_indices, dense_indices)\n",
        "        fusion_indices, fusion_scores = combsum_fusion(query_id, dense_indices, dense_scores, sparse_indices, sparse_scores)\n",
        "        print(fusion_indices, fusion_scores)\n",
        "        fusion_docs = [all_docs[idx]['doc_id'] for idx in fusion_indices]\n",
        "        results[\"rank_fusion\"].append({\"query\": query, \"results\": fusion_docs, \"scores\": fusion_scores})\n",
        "        # Rank Fusion Retrieval by combining sparse (BM25) and dense result\n",
        "        # fusion_indices, fusion_scores = fusion_retrieve(                                # Combine BM25 and cosine similarity results\n",
        "        #     query_embedding, doc_embeddings, query_text\n",
        "        # )\n",
        "        # fusion_docs = [all_docs[idx]['doc_id'] for idx in fusion_indices]\n",
        "        # results[\"rank_fusion\"].append({\"query\": query, \"results\": fusion_docs, \"scores\": fusion_scores})\n",
        "\n",
        "        # Cascade Retrieval: First use BM25, then re-rank using dense retrieval\n",
        "        cascade_indices, cascade_scores = cascade_retrieve(                             # Perform cascading retrieval\n",
        "            query_embedding, doc_embeddings, query_text\n",
        "        )\n",
        "        cascade_docs = [all_docs[idx]['doc_id'] for idx in cascade_indices]\n",
        "        results[\"cascade\"].append({\"query\": query, \"results\": cascade_docs, \"scores\": cascade_scores})\n",
        "\n",
        "    return results\n",
        "\n",
        "results = run_retrieval_experiments()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "JOAqgAfe5W6H",
      "metadata": {
        "id": "JOAqgAfe5W6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5b4fa6-38ec-43fe-a626-8e16f010c5e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated results: {\n",
            "    \"sparse\": {\n",
            "        \"recall_5\": 0.008603751543369476,\n",
            "        \"ndcg_cut_5\": 0.7017622426368895\n",
            "    },\n",
            "    \"dense\": {\n",
            "        \"recall_5\": 0.008045358532929462,\n",
            "        \"ndcg_cut_5\": 0.5830060175423573\n",
            "    },\n",
            "    \"rank_fusion\": {\n",
            "        \"recall_5\": 0.008561802785762561,\n",
            "        \"ndcg_cut_5\": 0.6735944159049838\n",
            "    },\n",
            "    \"cascade\": {\n",
            "        \"recall_5\": 0.008402741442359375,\n",
            "        \"ndcg_cut_5\": 0.6966364988247945\n",
            "    }\n",
            "}\n",
            "Retrieval results and metrics saved to files.\n"
          ]
        }
      ],
      "source": [
        "run_sparse = prepare_run_data(results[\"sparse\"])\n",
        "run_dense = prepare_run_data(results[\"dense\"])\n",
        "run_rank_fusion = prepare_run_data(results[\"rank_fusion\"])\n",
        "run_cascade = prepare_run_data(results[\"cascade\"])\n",
        "\n",
        "# Evaluate results with pytrec_eval\n",
        "evaluator = pytrec_eval.RelevanceEvaluator(qrels_dict, {'recall.5', 'ndcg_cut.5'})\n",
        "eval_results_sparse = evaluator.evaluate(run_sparse)\n",
        "eval_results_dense = evaluator.evaluate(run_dense)\n",
        "eval_results_rank_fusion = evaluator.evaluate(run_rank_fusion)\n",
        "eval_results_cascade = evaluator.evaluate(run_cascade)\n",
        "\n",
        "# Aggregate metrics for overall performance\n",
        "aggregated_results = {\n",
        "    \"sparse\": {\n",
        "        metric: sum([res[metric] for res in eval_results_sparse.values()]) / len(eval_results_sparse)\n",
        "        for metric in eval_results_sparse[next(iter(eval_results_sparse))]\n",
        "    },\n",
        "    \"dense\": {\n",
        "        metric: sum([res[metric] for res in eval_results_dense.values()]) / len(eval_results_dense)\n",
        "        for metric in eval_results_dense[next(iter(eval_results_dense))]\n",
        "    },\n",
        "    \"rank_fusion\": {\n",
        "        metric: sum([res[metric] for res in eval_results_rank_fusion.values()]) / len(eval_results_rank_fusion)\n",
        "        for metric in eval_results_rank_fusion[next(iter(eval_results_rank_fusion))]\n",
        "    },\n",
        "    \"cascade\": {\n",
        "        metric: sum([res[metric] for res in eval_results_cascade.values()]) / len(eval_results_cascade)\n",
        "        for metric in eval_results_cascade[next(iter(eval_results_cascade))]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Aggregated results:\", json.dumps(aggregated_results, indent=4))\n",
        "print(\"Retrieval results and metrics saved to files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "548ac9dd",
      "metadata": {
        "id": "548ac9dd"
      },
      "source": [
        "# Section 4: QA with Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee13b08-c67c-49f4-92cc-35c9ab5eaf5e",
      "metadata": {
        "id": "cee13b08-c67c-49f4-92cc-35c9ab5eaf5e"
      },
      "outputs": [],
      "source": [
        "# QA for the first query\n",
        "QUERY_INDEX = 3                                                     # Index of the query to be used for retrieval\n",
        "query = all_queries[QUERY_INDEX - 1]                                # Select the query from the list based on the index\n",
        "query_text = query['title'] if isinstance(query, dict) else query   # Get the query text\n",
        "\n",
        "# Retrieval calls:\n",
        "\n",
        "# Perform dense retrieval using query embedding and document embeddings\n",
        "dense_top_k_indices, dense_top_k_scores = dense_retrieve(query_embeddings[QUERY_INDEX], doc_embeddings)\n",
        "# Perform sparse retrieval using BM25 on the query text\n",
        "sparse_top_k_indices, sparse_top_k_scores = bm25_retrieve(query_text, bm25)\n",
        "# Perform rank fusion retrieval by combining BM25 and dense retrieval results\n",
        "rank_top_k_indices, rank_top_k_scores = fusion_retrieve(\n",
        "    query_embeddings[QUERY_INDEX],\n",
        "    doc_embeddings,\n",
        "    query_text\n",
        ")\n",
        "# Perform cascading retrieval: first BM25, then re-rank with dense retrieval\n",
        "cascading_top_k_indices, cascading_top_k_scores = cascade_retrieve(\n",
        "    query_embeddings[QUERY_INDEX],\n",
        "    doc_embeddings,\n",
        "    query_text\n",
        ")\n",
        "\n",
        "# Get retrieved documents for each method\n",
        "dense_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(dense_top_k_indices)]\n",
        "sparse_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(sparse_top_k_indices)]\n",
        "rank_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(rank_top_k_indices)]\n",
        "cascading_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(cascading_top_k_indices)]\n",
        "\n",
        "# Definition of the model that will be used to generate the various responses.\n",
        "lm_pipeline = pipeline(\"text-generation\",\n",
        "                      model=\"meta-llama/Llama-3.2-1B\",\n",
        "                      device=0 if device == \"cuda\" else -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30deea44",
      "metadata": {
        "id": "30deea44"
      },
      "source": [
        "#### Question-answering using DENSE RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54044429-a3c4-4f7b-8a5c-7baef2f13f3b",
      "metadata": {
        "id": "54044429-a3c4-4f7b-8a5c-7baef2f13f3b"
      },
      "outputs": [],
      "source": [
        "print(\"------------------ DENSE RETRIEVAL ----------------------\\n\")\n",
        "context = \"\\n\".join(dense_retrieved_docs)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "# Generate response\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b79a94b3",
      "metadata": {
        "id": "b79a94b3"
      },
      "source": [
        "#### Question-answering using SPARSE RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4220ff7d-8bee-48d8-ae1e-31547e158ebf",
      "metadata": {
        "id": "4220ff7d-8bee-48d8-ae1e-31547e158ebf"
      },
      "outputs": [],
      "source": [
        "print(\"------------------ SPARSE RETRIEVAL ----------------------\\n\")\n",
        "context = \"\\n\".join(sparse_retrieved_docs)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "# Generate response\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d52aca9e",
      "metadata": {
        "id": "d52aca9e"
      },
      "source": [
        "#### Question-answering using RANK FUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c3b1e8-d6b2-45df-84d0-e36413b0f6f8",
      "metadata": {
        "id": "51c3b1e8-d6b2-45df-84d0-e36413b0f6f8"
      },
      "outputs": [],
      "source": [
        "print(\"------------------ RANK FUSION ----------------------\\n\")\n",
        "context = \"\\n\".join(rank_retrieved_docs)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "# Generate response\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69d431c",
      "metadata": {
        "id": "e69d431c"
      },
      "source": [
        "#### Question-answering using CASCADING RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b0c87d-9e2f-4429-9a57-74d60f720fd1",
      "metadata": {
        "id": "d4b0c87d-9e2f-4429-9a57-74d60f720fd1"
      },
      "outputs": [],
      "source": [
        "print(\"------------------ CASCADING RETRIEVAL ----------------------\\n\")\n",
        "context = \"\\n\".join(cascading_retrieved_docs)\n",
        "prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "# Generate response\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8666f76",
      "metadata": {
        "id": "f8666f76"
      },
      "source": [
        "#### Question-answering WITH NO CONTEXT PROVIDED WITH RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6793863-6d60-46c2-983a-2d8d44fa15aa",
      "metadata": {
        "id": "d6793863-6d60-46c2-983a-2d8d44fa15aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"------------------ RESPONSE WITHOUT RAG ----------------------\\n\")\n",
        "prompt = f\"\"\"Question:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\"\"\n",
        "\n",
        "print(f\"----------------- Length of the prompt -----------------\\n{len(prompt.split())} words\")\n",
        "print(f\"------------------------ Prompt ------------------------\\n{prompt}\")\n",
        "\n",
        "response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.7,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "response = response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "print(f\"------------------ Response ------------------\\n{response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c28e3f7",
      "metadata": {
        "id": "6c28e3f7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "picked_queries = random.sample(all_queries, 5)\n",
        "\n",
        "for q in picked_queries:\n",
        "\n",
        "    # For each query, retrieve and rank documents independently\n",
        "    query_text = q['title']\n",
        "    cascading_top_k_indices, cascading_top_k_scores = cascade_retrieve(query_embeddings[QUERY_INDEX], doc_embeddings, query_text)\n",
        "\n",
        "    # Use the top-k documents for that specific query\n",
        "    cascading_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(cascading_top_k_indices)]\n",
        "    cascading_context = \"\\n\".join(cascading_retrieved_docs)\n",
        "\n",
        "    # Repeat the process for rank fusion\n",
        "    rank_top_k_indices, rank_top_k_scores = fusion_retrieve(query_embeddings[QUERY_INDEX], doc_embeddings, query_text)\n",
        "    rank_retrieved_docs = [f\"Document {i+1}: {all_docs[idx]['abstract']}\" for i, idx in enumerate(rank_top_k_indices)]\n",
        "    rank_fusion_context = \"\\n\".join(rank_retrieved_docs)\n",
        "\n",
        "\n",
        "    cascading_prompt = f\"Context:\\n{cascading_context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "    rank_fusion_prompt = f\"Context:\\n{rank_fusion_context}\\n\\nQuestion:\\n{query_text}\\n\\nAnswer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\"\n",
        "\n",
        "    # Generate response using language model\n",
        "    cascading_response = lm_pipeline(cascading_prompt,\n",
        "                           max_new_tokens=150,\n",
        "                           temperature=0.7,\n",
        "                           truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "    rank_fusion_response = lm_pipeline(prompt,\n",
        "                      max_new_tokens=150,\n",
        "                      temperature=0.1,\n",
        "                      truncation=False)[0][\"generated_text\"]\n",
        "\n",
        "    # Extract the answer from the response\n",
        "    cascading_response = cascading_response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "    rank_fusion_response = rank_fusion_response.split(\"Answer in a concise and clear manner without repetition (if no direct answer, provide a general summary):\")[1].strip()\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"\\nQuery: {query_text}\")\n",
        "    print(f\"Cascading Response: {cascading_response}\")\n",
        "    print(f\"Rank Fusion Response: {rank_fusion_response}\")\n",
        "    print(\"------------------------------\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}